# web_app.py
# -*- coding: utf-8 -*-
"""
AI_NovelGenerator Gradio Web Interface
åŸºäºåŸæœ‰GUIåŠŸèƒ½åˆ›å»ºçš„Webç•Œé¢ç‰ˆæœ¬
"""

import gradio as gr
import os
import json
import threading
import traceback
from typing import Optional, Tuple, Dict, Any

# å¯¼å…¥åŸæœ‰çš„æ ¸å¿ƒåŠŸèƒ½æ¨¡å—
from config_manager import load_config, save_config, test_llm_config, test_embedding_config
from novel_generator import (
    Novel_architecture_generate,
    Chapter_blueprint_generate,
    generate_chapter_draft,
    finalize_chapter,
    import_knowledge_file,
    clear_vector_store
)
from consistency_checker import check_consistency
from utils import read_file, save_string_to_txt, clear_file_content
from llm_adapters import create_llm_adapter
from embedding_adapters import create_embedding_adapter

class NovelGeneratorWebApp:
    """AIå°è¯´ç”Ÿæˆå™¨Webåº”ç”¨ç±»"""

    def __init__(self):
        self.config_file = "config.json"
        self.loaded_config = load_config(self.config_file)

        # åˆå§‹åŒ–é»˜è®¤é…ç½®
        self.init_default_config()

        # çŠ¶æ€å˜é‡
        self.current_chapter_num = 1
        self.generation_in_progress = False

    def init_default_config(self):
        """åˆå§‹åŒ–é»˜è®¤é…ç½®"""
        if self.loaded_config:
            self.last_llm = self.loaded_config.get("last_interface_format", "OpenAI")
            self.last_embedding = self.loaded_config.get("last_embedding_interface_format", "OpenAI")
        else:
            self.last_llm = "OpenAI"
            self.last_embedding = "OpenAI"

    def log_message(self, message: str) -> str:
        """æ·»åŠ æ—¥å¿—æ¶ˆæ¯"""
        import datetime
        timestamp = datetime.datetime.now().strftime("%H:%M:%S")
        return f"[{timestamp}] {message}\n"

    def get_llm_config_from_form(self, interface_format, api_key, base_url, model_name,
                                temperature, max_tokens, timeout):
        """ä»è¡¨å•è·å–LLMé…ç½®"""
        return {
            "interface_format": interface_format,
            "api_key": api_key,
            "base_url": base_url,
            "model_name": model_name,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "timeout": timeout
        }

    def get_embedding_config_from_form(self, interface_format, api_key, base_url,
                                     model_name, retrieval_k):
        """ä»è¡¨å•è·å–Embeddingé…ç½®"""
        return {
            "interface_format": interface_format,
            "api_key": api_key,
            "base_url": base_url,
            "model_name": model_name,
            "retrieval_k": retrieval_k
        }

    def save_config_to_file(self, llm_config, embedding_config, novel_params):
        """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
        try:
            config_data = {
                "last_interface_format": llm_config["interface_format"],
                "last_embedding_interface_format": embedding_config["interface_format"],
                "llm_configs": {
                    llm_config["interface_format"]: {
                        "api_key": llm_config["api_key"],
                        "base_url": llm_config["base_url"],
                        "model_name": llm_config["model_name"],
                        "temperature": llm_config["temperature"],
                        "max_tokens": llm_config["max_tokens"],
                        "timeout": llm_config["timeout"]
                    }
                },
                "embedding_configs": {
                    embedding_config["interface_format"]: {
                        "api_key": embedding_config["api_key"],
                        "base_url": embedding_config["base_url"],
                        "model_name": embedding_config["model_name"],
                        "retrieval_k": embedding_config["retrieval_k"]
                    }
                },
                "other_params": novel_params
            }

            success = save_config(config_data, self.config_file)
            if success:
                self.loaded_config = config_data
                return "âœ… é…ç½®ä¿å­˜æˆåŠŸï¼"
            else:
                return "âŒ é…ç½®ä¿å­˜å¤±è´¥ï¼"
        except Exception as e:
            return f"âŒ ä¿å­˜é…ç½®æ—¶å‡ºé”™: {str(e)}"

    def load_config_from_file(self):
        """ä»æ–‡ä»¶åŠ è½½é…ç½®"""
        try:
            cfg = load_config(self.config_file)
            if not cfg:
                return None, "æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶"

            # æå–LLMé…ç½®
            last_llm = cfg.get("last_interface_format", "OpenAI")
            llm_configs = cfg.get("llm_configs", {})
            llm_config = llm_configs.get(last_llm, {})

            # æå–Embeddingé…ç½®
            last_embedding = cfg.get("last_embedding_interface_format", "OpenAI")
            embedding_configs = cfg.get("embedding_configs", {})
            embedding_config = embedding_configs.get(last_embedding, {})

            # æå–å…¶ä»–å‚æ•°
            other_params = cfg.get("other_params", {})

            return {
                "llm_interface": last_llm,
                "llm_api_key": llm_config.get("api_key", ""),
                "llm_base_url": llm_config.get("base_url", "https://api.openai.com/v1"),
                "llm_model": llm_config.get("model_name", "gpt-4o-mini"),
                "temperature": llm_config.get("temperature", 0.7),
                "max_tokens": llm_config.get("max_tokens", 8192),
                "timeout": llm_config.get("timeout", 600),
                "embedding_interface": last_embedding,
                "embedding_api_key": embedding_config.get("api_key", ""),
                "embedding_base_url": embedding_config.get("base_url", "https://api.openai.com/v1"),
                "embedding_model": embedding_config.get("model_name", "text-embedding-ada-002"),
                "retrieval_k": embedding_config.get("retrieval_k", 4),
                "topic": other_params.get("topic", ""),
                "genre": other_params.get("genre", "ç„å¹»"),
                "num_chapters": other_params.get("num_chapters", 10),
                "word_number": other_params.get("word_number", 3000),
                "filepath": other_params.get("filepath", ""),
                "user_guidance": other_params.get("user_guidance", ""),
                "characters_involved": other_params.get("characters_involved", ""),
                "key_items": other_params.get("key_items", ""),
                "scene_location": other_params.get("scene_location", ""),
                "time_constraint": other_params.get("time_constraint", "")
            }, "âœ… é…ç½®åŠ è½½æˆåŠŸï¼"
        except Exception as e:
            return None, f"âŒ åŠ è½½é…ç½®æ—¶å‡ºé”™: {str(e)}"

# åˆ›å»ºå…¨å±€åº”ç”¨å®ä¾‹
app = NovelGeneratorWebApp()

def create_interface():
    """åˆ›å»ºGradioç•Œé¢"""

    # å®šä¹‰LLMæ¥å£é€‰é¡¹
    llm_interfaces = ["OpenAI", "DeepSeek", "Azure OpenAI", "Azure AI", "Ollama",
                     "ML Studio", "Gemini", "é˜¿é‡Œäº‘ç™¾ç‚¼", "ç«å±±å¼•æ“", "ç¡…åŸºæµåŠ¨"]

    # å®šä¹‰ç±»å‹é€‰é¡¹
    genres = ["ç„å¹»", "ä»™ä¾ ", "éƒ½å¸‚", "å†å²", "ç§‘å¹»", "å†›äº‹", "æ¸¸æˆ", "ä½“è‚²", "æ‚¬ç–‘", "å…¶ä»–"]

    # è‡ªåŠ¨åŠ è½½å·²ä¿å­˜çš„é…ç½®
    config_data, config_message = app.load_config_from_file()
    if config_data:
        print(f"âœ… è‡ªåŠ¨åŠ è½½é…ç½®æˆåŠŸ: {config_message}")
        # ä½¿ç”¨åŠ è½½çš„é…ç½®ä½œä¸ºé»˜è®¤å€¼
        default_llm_interface = config_data["llm_interface"]
        default_llm_api_key = config_data["llm_api_key"]
        default_llm_base_url = config_data["llm_base_url"]
        default_llm_model = config_data["llm_model"]
        default_temperature = config_data["temperature"]
        default_max_tokens = config_data["max_tokens"]
        default_timeout = config_data["timeout"]
        default_embedding_interface = config_data["embedding_interface"]
        default_embedding_api_key = config_data["embedding_api_key"]
        default_embedding_base_url = config_data["embedding_base_url"]
        default_embedding_model = config_data["embedding_model"]
        default_retrieval_k = config_data["retrieval_k"]
        default_topic = config_data["topic"]
        default_genre = config_data["genre"]
        default_num_chapters = config_data["num_chapters"]
        default_word_number = config_data["word_number"]
        default_filepath = config_data["filepath"]
        default_user_guidance = config_data["user_guidance"]
        default_characters_involved = config_data["characters_involved"]
        default_key_items = config_data["key_items"]
        default_scene_location = config_data["scene_location"]
        default_time_constraint = config_data["time_constraint"]
    else:
        print(f"âš ï¸ æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤å€¼: {config_message}")
        # ä½¿ç”¨é»˜è®¤å€¼
        default_llm_interface = "OpenAI"
        default_llm_api_key = ""
        default_llm_base_url = "https://api.openai.com/v1"
        default_llm_model = "gpt-4o-mini"
        default_temperature = 0.7
        default_max_tokens = 8192
        default_timeout = 600
        default_embedding_interface = "OpenAI"
        default_embedding_api_key = ""
        default_embedding_base_url = "https://api.openai.com/v1"
        default_embedding_model = "text-embedding-ada-002"
        default_retrieval_k = 4
        default_topic = ""
        default_genre = "ç„å¹»"
        default_num_chapters = 10
        default_word_number = 3000
        default_filepath = ""
        default_user_guidance = ""
        default_characters_involved = ""
        default_key_items = ""
        default_scene_location = ""
        default_time_constraint = ""

    # åˆ›å»ºè‡ªå®šä¹‰CSSæ ·å¼
    custom_css = """
    /* å…¨å±€æ ·å¼ - ç¡®ä¿æ‰€æœ‰é¡µé¢å®½åº¦ä¸€è‡´ */
    .gradio-container {
        max-width: 1400px !important;
        margin: 0 auto !important;
        padding: 0 1rem !important;
    }

    /* ç¡®ä¿æ‰€æœ‰æ ‡ç­¾é¡µå†…å®¹å®½åº¦ä¸€è‡´ */
    .tab-content {
        width: 100% !important;
        max-width: 1400px !important;
        margin: 0 auto !important;
        padding: 1rem !important;
    }

    /* æ ‡ç­¾é¡µå®¹å™¨ */
    .gradio-tabs {
        width: 100% !important;
    }

    .gradio-tabitem {
        width: 100% !important;
        max-width: 1400px !important;
        margin: 0 auto !important;
        padding: 1rem !important;
    }

    /* è¡Œå’Œåˆ—çš„ä¸€è‡´æ€§ */
    .gradio-row {
        width: 100% !important;
        margin: 0 !important;
        gap: 1rem !important;
    }

    .gradio-column {
        width: 100% !important;
    }

    /* ç¡®ä¿æ‰€æœ‰æ ‡ç­¾é¡µå†…å®¹å®½åº¦ä¸€è‡´ */
    .gradio-tabitem {
        max-width: 1400px !important;
        margin: 0 auto !important;
        width: 100% !important;
    }

    /* æ ‡é¢˜æ ·å¼ */
    .main-header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 2rem;
        border-radius: 15px;
        margin-bottom: 2rem;
        text-align: center;
        box-shadow: 0 8px 32px rgba(0,0,0,0.1);
        width: 100% !important;
        max-width: 1400px !important;
        margin-left: auto !important;
        margin-right: auto !important;
    }

    .main-title {
        font-size: 2.5rem;
        font-weight: bold;
        margin-bottom: 0.5rem;
        text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
    }

    .main-subtitle {
        font-size: 1.2rem;
        opacity: 0.9;
        margin-bottom: 1rem;
    }

    /* å¡ç‰‡æ ·å¼ */
    .feature-card {
        background: white;
        border-radius: 12px;
        padding: 1.5rem;
        margin: 1rem 0;
        box-shadow: 0 4px 16px rgba(0,0,0,0.1);
        border: 1px solid #e1e5e9;
        transition: transform 0.3s ease, box-shadow 0.3s ease;
        width: 100% !important;
    }

    .feature-card:hover {
        transform: translateY(-2px);
        box-shadow: 0 8px 24px rgba(0,0,0,0.15);
    }

    /* æŒ‰é’®æ ·å¼ */
    .primary-button {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%) !important;
        border: none !important;
        border-radius: 8px !important;
        padding: 12px 24px !important;
        font-weight: 600 !important;
        transition: all 0.3s ease !important;
    }

    .primary-button:hover {
        transform: translateY(-1px) !important;
        box-shadow: 0 4px 12px rgba(102, 126, 234, 0.4) !important;
    }

    /* æ ‡ç­¾é¡µæ ·å¼ */
    .tab-nav {
        background: #f8f9fa;
        border-radius: 10px;
        padding: 0.5rem;
        margin-bottom: 1rem;
    }

    /* æ—¥å¿—çª—å£æ ·å¼ */
    .log-container {
        background: #1e1e1e;
        color: #00ff00;
        font-family: 'Courier New', monospace;
        border-radius: 8px;
        padding: 1rem;
    }

    /* é…ç½®åŒºåŸŸæ ·å¼ */
    .config-section {
        background: #f8f9fa;
        border-radius: 12px;
        padding: 1.5rem;
        margin: 1rem 0;
        border-left: 4px solid #667eea;
        width: 100% !important;
        box-sizing: border-box !important;
    }

    /* æ–‡æœ¬æ¡†å’Œè¾“å…¥ç»„ä»¶ç»Ÿä¸€å®½åº¦ */
    .gradio-textbox, .gradio-dropdown, .gradio-number, .gradio-slider {
        width: 100% !important;
    }

    /* ç¡®ä¿æ‰€æœ‰è¾“å…¥ç»„ä»¶å®¹å™¨å®½åº¦ä¸€è‡´ */
    .gradio-form > * {
        width: 100% !important;
        margin-bottom: 1rem !important;
    }

    /* çŠ¶æ€æŒ‡ç¤ºå™¨ */
    .status-indicator {
        display: inline-block;
        width: 12px;
        height: 12px;
        border-radius: 50%;
        margin-right: 8px;
    }

    .status-success { background-color: #28a745; }
    .status-warning { background-color: #ffc107; }
    .status-error { background-color: #dc3545; }

    /* ç¡®ä¿æŒ‰é’®å®¹å™¨å®½åº¦ä¸€è‡´ */
    .gradio-button {
        width: 100% !important;
        margin: 0.5rem 0 !important;
    }

    /* æ—¥å¿—å®¹å™¨æ ·å¼ç»Ÿä¸€ */
    .log-container {
        width: 100% !important;
        max-width: 100% !important;
    }

    /* å“åº”å¼è®¾è®¡ */
    @media (max-width: 768px) {
        .gradio-container {
            max-width: 100% !important;
            padding: 0 0.5rem !important;
        }

        .main-header {
            padding: 1.5rem !important;
            margin-bottom: 1rem !important;
        }

        .main-title {
            font-size: 2rem !important;
        }

        .main-subtitle {
            font-size: 1rem !important;
        }

        .feature-card {
            margin: 0.5rem 0 !important;
            padding: 1rem !important;
        }

        .config-section {
            padding: 1rem !important;
            margin: 0.5rem 0 !important;
        }

        .gradio-tabitem {
            padding: 0.5rem !important;
        }
    }
    """

    with gr.Blocks(
        title="AIå°è¯´ç”Ÿæˆå™¨ - æ™ºèƒ½åˆ›ä½œå¹³å°",
        theme=gr.themes.Soft(),
        css=custom_css
    ) as demo:

        # Landingé¡µé¢å¤´éƒ¨
        with gr.Row():
            with gr.Column():
                gr.HTML("""
                <div class="main-header">
                    <div class="main-title">ğŸ¯ AIå°è¯´ç”Ÿæˆå™¨</div>
                    <div class="main-subtitle">åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½å°è¯´åˆ›ä½œå¹³å°</div>
                    <div style="margin-top: 1rem;">
                        <span style="background: rgba(255,255,255,0.2); padding: 0.5rem 1rem; border-radius: 20px; margin: 0 0.5rem;">
                            âœ¨ æ™ºèƒ½æ¶æ„ç”Ÿæˆ
                        </span>
                        <span style="background: rgba(255,255,255,0.2); padding: 0.5rem 1rem; border-radius: 20px; margin: 0 0.5rem;">
                            ğŸ“š ç« èŠ‚è“å›¾è§„åˆ’
                        </span>
                        <span style="background: rgba(255,255,255,0.2); padding: 0.5rem 1rem; border-radius: 20px; margin: 0 0.5rem;">
                            ğŸ” æ™ºèƒ½ä¸Šä¸‹æ–‡æ£€ç´¢
                        </span>
                    </div>
                </div>
                """)

        # ä¸»è¦çŠ¶æ€å˜é‡
        log_state = gr.State("")

        with gr.Tabs() as tabs:
            # Tab 0: æ¬¢è¿é¡µé¢
            with gr.Tab("ğŸ  æ¬¢è¿", id="welcome"):
                with gr.Column(elem_classes=["tab-content"]):
                    with gr.Row():
                        with gr.Column(scale=2):
                            gr.HTML("""
                            <div class="feature-card">
                                <h2>ğŸš€ å¿«é€Ÿå¼€å§‹</h2>
                                <p>æ¬¢è¿ä½¿ç”¨AIå°è¯´ç”Ÿæˆå™¨ï¼æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å¼€å§‹æ‚¨çš„åˆ›ä½œä¹‹æ—…ï¼š</p>
                                <ol style="line-height: 1.8;">
                                    <li><strong>é…ç½®æ¨¡å‹</strong> - å‰å¾€"æ¨¡å‹é…ç½®"é¡µé¢è®¾ç½®æ‚¨çš„AIæ¨¡å‹</li>
                                    <li><strong>è®¾ç½®å‚æ•°</strong> - åœ¨"å°è¯´å‚æ•°"é¡µé¢å¡«å†™å°è¯´åŸºæœ¬ä¿¡æ¯</li>
                                    <li><strong>å¼€å§‹åˆ›ä½œ</strong> - åœ¨"ä¸»è¦åŠŸèƒ½"é¡µé¢æŒ‰æ­¥éª¤ç”Ÿæˆå°è¯´</li>
                                    <li><strong>ç®¡ç†æ–‡ä»¶</strong> - ä½¿ç”¨"æ–‡ä»¶ç®¡ç†"é¡µé¢æŸ¥çœ‹å’Œç¼–è¾‘å†…å®¹</li>
                                </ol>
                            </div>
                            """)

                        with gr.Row():
                            with gr.Column():
                                gr.HTML("""
                                <div class="feature-card">
                                    <h3>âœ¨ æ ¸å¿ƒåŠŸèƒ½</h3>
                                    <ul style="line-height: 1.6;">
                                        <li><strong>æ™ºèƒ½æ¶æ„ç”Ÿæˆ</strong> - AIè‡ªåŠ¨æ„å»ºå°è¯´ä¸–ç•Œè§‚å’Œæƒ…èŠ‚æ¡†æ¶</li>
                                        <li><strong>ç« èŠ‚è“å›¾è§„åˆ’</strong> - è¯¦ç»†çš„ç« èŠ‚å¤§çº²å’Œå‰§æƒ…å®‰æ’</li>
                                        <li><strong>è‰ç¨¿æ™ºèƒ½ç”Ÿæˆ</strong> - åŸºäºä¸Šä¸‹æ–‡çš„ç« èŠ‚å†…å®¹åˆ›ä½œ</li>
                                        <li><strong>ä¸€è‡´æ€§æ£€æŸ¥</strong> - è‡ªåŠ¨æ£€æµ‹å‰§æƒ…é€»è¾‘å’Œè§’è‰²ä¸€è‡´æ€§</li>
                                    </ul>
                                </div>
                                """)

                            with gr.Column():
                                gr.HTML("""
                                <div class="feature-card">
                                    <h3>ğŸ¯ æ”¯æŒçš„æ¨¡å‹</h3>
                                    <ul style="line-height: 1.6;">
                                        <li><strong>OpenAI</strong> - GPT-4, GPT-3.5ç³»åˆ—</li>
                                        <li><strong>Google Gemini</strong> - Gemini Pro, Flashç³»åˆ—</li>
                                        <li><strong>DeepSeek</strong> - DeepSeek Chatç³»åˆ—</li>
                                        <li><strong>ç«å±±å¼•æ“</strong> - è±†åŒ…ç³»åˆ—æ¨¡å‹</li>
                                        <li><strong>å…¶ä»–</strong> - Azure OpenAI, Ollamaç­‰</li>
                                    </ul>
                                </div>
                                """)

                    with gr.Column(scale=1):
                        gr.HTML("""
                        <div class="feature-card">
                            <h3>ğŸ“Š ç³»ç»ŸçŠ¶æ€</h3>
                            <div style="margin: 1rem 0;">
                                <div style="margin: 0.5rem 0;">
                                    <span class="status-indicator status-warning"></span>
                                    <span>é…ç½®çŠ¶æ€: å¾…è®¾ç½®</span>
                                </div>
                                <div style="margin: 0.5rem 0;">
                                    <span class="status-indicator status-warning"></span>
                                    <span>æ¨¡å‹è¿æ¥: æœªæµ‹è¯•</span>
                                </div>
                                <div style="margin: 0.5rem 0;">
                                    <span class="status-indicator status-success"></span>
                                    <span>ç³»ç»ŸçŠ¶æ€: æ­£å¸¸</span>
                                </div>
                            </div>
                        </div>
                        """)

                        gr.HTML("""
                        <div class="feature-card">
                            <h3>ğŸ’¡ ä½¿ç”¨æç¤º</h3>
                            <ul style="line-height: 1.6; font-size: 0.9rem;">
                                <li>é¦–æ¬¡ä½¿ç”¨è¯·å…ˆé…ç½®APIå¯†é’¥</li>
                                <li>å»ºè®®å…ˆæµ‹è¯•æ¨¡å‹è¿æ¥</li>
                                <li>è®¾ç½®åˆé€‚çš„ä¿å­˜è·¯å¾„</li>
                                <li>å¯ä»¥éšæ—¶ç¼–è¾‘ç”Ÿæˆçš„å†…å®¹</li>
                                <li>æ”¯æŒå¤šç§æ–‡ä»¶æ ¼å¼å¯¼å‡º</li>
                            </ul>
                        </div>
                        """)

                        # å¿«é€Ÿé…ç½®æŒ‰é’®
                        with gr.Row():
                            quick_config_btn = gr.Button("ğŸ”§ å¿«é€Ÿé…ç½®", variant="primary", elem_classes=["primary-button"])
                            quick_start_btn = gr.Button("ğŸš€ å¼€å§‹åˆ›ä½œ", variant="primary", elem_classes=["primary-button"])

            # Tab 1: ä¸»è¦åŠŸèƒ½
            with gr.Tab("ğŸ“ ä¸»è¦åŠŸèƒ½", id="main"):
                with gr.Column(elem_classes=["tab-content"]):
                    with gr.Row():
                        with gr.Column(scale=2):
                            # ç« èŠ‚å†…å®¹åŒºåŸŸ
                            gr.HTML("""
                            <div class="config-section">
                                <h3>ğŸ“– å½“å‰ç« èŠ‚å†…å®¹</h3>
                                <p>åœ¨è¿™é‡ŒæŸ¥çœ‹å’Œç¼–è¾‘ç”Ÿæˆçš„ç« èŠ‚å†…å®¹</p>
                            </div>
                            """)
                        chapter_content = gr.Textbox(
                            label="ç« èŠ‚å†…å®¹ï¼ˆå¯ç¼–è¾‘ï¼‰",
                            lines=15,
                            max_lines=20,
                            placeholder="ç”Ÿæˆçš„ç« èŠ‚å†…å®¹å°†æ˜¾ç¤ºåœ¨è¿™é‡Œ...\n\nğŸ’¡ æç¤ºï¼š\n1. å†…å®¹ç”Ÿæˆåå¯ä»¥ç›´æ¥ç¼–è¾‘\n2. ç¼–è¾‘åçš„å†…å®¹ä¼šåœ¨å®šç¨¿æ—¶ä¿å­˜\n3. æ”¯æŒMarkdownæ ¼å¼",
                            interactive=True
                        )

                        # StepæŒ‰é’®åŒºåŸŸ
                        gr.HTML("""
                        <div class="config-section">
                            <h3>ğŸš€ æ™ºèƒ½ç”Ÿæˆæµç¨‹</h3>
                            <p>æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å®Œæˆå°è¯´åˆ›ä½œï¼Œæ¯ä¸ªæ­¥éª¤éƒ½ä¼šåŸºäºå‰ä¸€æ­¥çš„ç»“æœ</p>
                        </div>
                        """)
                        with gr.Row():
                            btn_step1 = gr.Button("Step1. ç”Ÿæˆæ¶æ„", variant="primary", elem_classes=["primary-button"])
                            btn_step2 = gr.Button("Step2. ç”Ÿæˆç›®å½•", variant="primary", elem_classes=["primary-button"])
                            btn_step3 = gr.Button("Step3. ç”Ÿæˆè‰ç¨¿", variant="primary", elem_classes=["primary-button"])
                            btn_step4 = gr.Button("Step4. å®šç¨¿ç« èŠ‚", variant="primary", elem_classes=["primary-button"])

                        # æ­¥éª¤è¯´æ˜
                        gr.HTML("""
                        <div style="margin: 1rem 0; padding: 1rem; background: #f8f9fa; border-radius: 8px; font-size: 0.9rem;">
                            <strong>ğŸ“‹ æµç¨‹è¯´æ˜ï¼š</strong><br>
                            <strong>Step1</strong> - æ ¹æ®ä¸»é¢˜ç”Ÿæˆå®Œæ•´çš„å°è¯´æ¶æ„å’Œä¸–ç•Œè§‚<br>
                            <strong>Step2</strong> - åŸºäºæ¶æ„ç”Ÿæˆè¯¦ç»†çš„ç« èŠ‚è“å›¾<br>
                            <strong>Step3</strong> - ç”ŸæˆæŒ‡å®šç« èŠ‚çš„è‰ç¨¿å†…å®¹<br>
                            <strong>Step4</strong> - å®šç¨¿ç« èŠ‚å¹¶æ›´æ–°å…¨å±€çŠ¶æ€
                        </div>
                        """)

                        # è¾…åŠ©åŠŸèƒ½æŒ‰é’®
                        gr.HTML("""
                        <div class="config-section">
                            <h3>ğŸ”§ è¾…åŠ©å·¥å…·</h3>
                            <p>æä¾›é¢å¤–çš„åˆ›ä½œè¾…åŠ©åŠŸèƒ½</p>
                        </div>
                        """)
                        with gr.Row():
                            btn_consistency = gr.Button("ğŸ” ä¸€è‡´æ€§æ£€æŸ¥", elem_classes=["primary-button"])
                            btn_import_knowledge = gr.Button("ğŸ“š å¯¼å…¥çŸ¥è¯†åº“", elem_classes=["primary-button"])
                            btn_clear_vectorstore = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå‘é‡åº“", variant="stop")
                            btn_plot_arcs = gr.Button("ğŸ“Š æŸ¥çœ‹å‰§æƒ…è¦ç‚¹", elem_classes=["primary-button"])

                        # æ—¥å¿—åŒºåŸŸ
                        gr.HTML("""
                        <div class="config-section">
                            <h3>ğŸ“‹ ç³»ç»Ÿæ—¥å¿—</h3>
                            <p>å®æ—¶æ˜¾ç¤ºç”Ÿæˆè¿‡ç¨‹å’Œç³»ç»ŸçŠ¶æ€</p>
                        </div>
                        """)
                        log_output = gr.Textbox(
                            label="ç³»ç»Ÿæ—¥å¿—",
                            lines=8,
                            max_lines=10,
                            interactive=False,
                            value="",
                            elem_classes=["log-container"]
                        )

                    with gr.Column(scale=1):
                        # å³ä¾§ï¼šé…ç½®å’Œå‚æ•°
                        gr.HTML("""
                        <div class="config-section">
                            <h3>âš™ï¸ å¿«é€Ÿé…ç½®</h3>
                            <p>è®¾ç½®å½“å‰åˆ›ä½œçš„åŸºæœ¬å‚æ•°</p>
                        </div>
                        """)

                        # æ–‡ä»¶è·¯å¾„è®¾ç½®
                        filepath_input = gr.Textbox(
                            label="ğŸ“ ä¿å­˜è·¯å¾„",
                            placeholder="ä¾‹å¦‚: /Users/username/novels/my_novel",
                            value=default_filepath,
                            info="å°è¯´æ–‡ä»¶çš„ä¿å­˜ç›®å½•"
                        )

                        # ç« èŠ‚å·è®¾ç½®
                        chapter_num_input = gr.Number(
                            label="ğŸ“– å½“å‰ç« èŠ‚å·",
                            value=1,
                            minimum=1,
                            step=1,
                            info="è¦ç”Ÿæˆæˆ–ç¼–è¾‘çš„ç« èŠ‚ç¼–å·"
                        )

                        # æœ¬ç« æŒ‡å¯¼
                        user_guidance_input = gr.Textbox(
                            label="ğŸ’¡ æœ¬ç« æŒ‡å¯¼",
                            lines=4,
                            value=default_user_guidance,
                            placeholder="ä¾‹å¦‚ï¼š\n- ä¸»è§’é‡åˆ°ç¥ç§˜è€äºº\n- æ­ç¤ºé‡è¦çº¿ç´¢\n- è¥é€ ç´§å¼ æ°›å›´\n- ä¸ºä¸‹ç« åŸ‹ä¸‹ä¼ç¬”",
                            info="å¯¹æœ¬ç« å‰§æƒ…å‘å±•çš„å…·ä½“è¦æ±‚å’Œæç¤º"
                        )

                        # é…ç½®ç®¡ç†
                        gr.HTML("""
                        <div class="config-section">
                            <h3>ğŸ’¾ é…ç½®ç®¡ç†</h3>
                            <p>ä¿å­˜å’ŒåŠ è½½åˆ›ä½œé…ç½®</p>
                        </div>
                        """)
                        with gr.Row():
                            btn_load_config = gr.Button("ğŸ“¥ åŠ è½½é…ç½®", elem_classes=["primary-button"])
                            btn_save_config = gr.Button("ğŸ’¾ ä¿å­˜é…ç½®", elem_classes=["primary-button"])

                        # åˆ›ä½œè¿›åº¦
                        gr.HTML("""
                        <div class="feature-card">
                            <h4>ğŸ“Š åˆ›ä½œè¿›åº¦</h4>
                            <div style="margin: 0.5rem 0;">
                                <div style="background: #e9ecef; border-radius: 10px; height: 8px; overflow: hidden;">
                                    <div style="background: linear-gradient(90deg, #667eea, #764ba2); height: 100%; width: 0%; transition: width 0.3s ease;"></div>
                                </div>
                                <small style="color: #6c757d;">0% å®Œæˆ (0/10 ç« èŠ‚)</small>
                            </div>
                        </div>
                        """)

                        # å¿«æ·æ“ä½œ
                        gr.HTML("""
                        <div class="feature-card">
                            <h4>âš¡ å¿«æ·æ“ä½œ</h4>
                            <div style="font-size: 0.9rem; line-height: 1.6;">
                                <div>ğŸ”§ <strong>Ctrl+S</strong> - ä¿å­˜å½“å‰å†…å®¹</div>
                                <div>ğŸ“ <strong>Ctrl+E</strong> - ç¼–è¾‘æ¨¡å¼</div>
                                <div>ğŸ”„ <strong>Ctrl+R</strong> - é‡æ–°ç”Ÿæˆ</div>
                                <div>ğŸ’¾ <strong>Ctrl+Shift+S</strong> - å¯¼å‡ºæ–‡ä»¶</div>
                            </div>
                        </div>
                        """)

            # Tab 2: è¯¦ç»†é…ç½®
            with gr.Tab("ğŸ”§ æ¨¡å‹é…ç½®", id="config"):
                with gr.Column(elem_classes=["tab-content"]):
                    with gr.Row():
                        with gr.Column(scale=2):
                            with gr.Row():
                                with gr.Column():
                                    gr.HTML("""
                                    <div class="config-section">
                                        <h3>ğŸ¤– LLMæ¨¡å‹è®¾ç½®</h3>
                                        <p>é…ç½®ç”¨äºæ–‡æœ¬ç”Ÿæˆçš„å¤§è¯­è¨€æ¨¡å‹</p>
                                    </div>
                                    """)
                                llm_interface = gr.Dropdown(
                                    choices=llm_interfaces,
                                    label="ğŸ”Œ æ¥å£ç±»å‹",
                                    value=default_llm_interface,
                                    info="é€‰æ‹©AIæœåŠ¡æä¾›å•†"
                                )
                                llm_api_key = gr.Textbox(
                                    label="ğŸ”‘ API Key",
                                    type="password",
                                    value=default_llm_api_key,
                                    placeholder="è¯·è¾“å…¥APIå¯†é’¥",
                                    info="ä»æœåŠ¡å•†è·å–çš„APIå¯†é’¥"
                                )
                                llm_base_url = gr.Textbox(
                                    label="ğŸŒ Base URL",
                                    value=default_llm_base_url,
                                    placeholder="APIåŸºç¡€URL",
                                    info="APIæœåŠ¡çš„åŸºç¡€åœ°å€"
                                )
                                llm_model = gr.Textbox(
                                    label="ğŸ¯ æ¨¡å‹åç§°",
                                    value=default_llm_model,
                                    placeholder="æ¨¡å‹åç§°",
                                    info="å…·ä½“çš„æ¨¡å‹æ ‡è¯†ç¬¦"
                                )

                                gr.HTML("""
                                <div style="margin: 1rem 0;">
                                    <h4>âš™ï¸ ç”Ÿæˆå‚æ•°</h4>
                                </div>
                                """)
                                with gr.Row():
                                    temperature = gr.Slider(
                                        label="ğŸŒ¡ï¸ Temperature",
                                        minimum=0.0,
                                        maximum=2.0,
                                        value=default_temperature,
                                        step=0.1,
                                        info="æ§åˆ¶è¾“å‡ºçš„éšæœºæ€§ (0.0-2.0)"
                                    )
                                    max_tokens = gr.Number(
                                        label="ğŸ“ Max Tokens",
                                        value=default_max_tokens,
                                        minimum=1,
                                        info="å•æ¬¡ç”Ÿæˆçš„æœ€å¤§å­—ç¬¦æ•°"
                                    )
                                    timeout = gr.Number(
                                        label="â±ï¸ Timeout (ç§’)",
                                        value=default_timeout,
                                        minimum=1,
                                        info="è¯·æ±‚è¶…æ—¶æ—¶é—´"
                                    )

                                btn_test_llm = gr.Button("ğŸ§ª æµ‹è¯•LLMé…ç½®", variant="primary", elem_classes=["primary-button"])

                            with gr.Column():
                                gr.HTML("""
                                <div class="config-section">
                                    <h3>ğŸ” Embeddingæ¨¡å‹è®¾ç½®</h3>
                                    <p>é…ç½®ç”¨äºå‘é‡æ£€ç´¢çš„åµŒå…¥æ¨¡å‹</p>
                                </div>
                                """)
                                embedding_interface = gr.Dropdown(
                                    choices=llm_interfaces,
                                    label="ğŸ”Œ æ¥å£ç±»å‹",
                                    value=default_embedding_interface,
                                    info="é€‰æ‹©EmbeddingæœåŠ¡æä¾›å•†"
                                )
                                embedding_api_key = gr.Textbox(
                                    label="ğŸ”‘ API Key",
                                    type="password",
                                    value=default_embedding_api_key,
                                    placeholder="è¯·è¾“å…¥APIå¯†é’¥",
                                    info="å¯ä¸LLMä½¿ç”¨ç›¸åŒå¯†é’¥"
                                )
                                embedding_base_url = gr.Textbox(
                                    label="ğŸŒ Base URL",
                                    value=default_embedding_base_url,
                                    placeholder="APIåŸºç¡€URL",
                                    info="Embedding APIçš„åŸºç¡€åœ°å€"
                                )
                                embedding_model = gr.Textbox(
                                    label="ğŸ¯ æ¨¡å‹åç§°",
                                    value=default_embedding_model,
                                    placeholder="Embeddingæ¨¡å‹åç§°",
                                    info="å‘é‡åŒ–æ¨¡å‹çš„æ ‡è¯†ç¬¦"
                                )

                                gr.HTML("""
                                <div style="margin: 1rem 0;">
                                    <h4>ğŸ”§ æ£€ç´¢å‚æ•°</h4>
                                </div>
                                """)
                                retrieval_k = gr.Number(
                                    label="ğŸ“Š æ£€ç´¢æ•°é‡ (K)",
                                    value=default_retrieval_k,
                                    minimum=1,
                                    maximum=20,
                                    info="æ¯æ¬¡æ£€ç´¢è¿”å›çš„ç›¸å…³ç‰‡æ®µæ•°é‡"
                                )

                                btn_test_embedding = gr.Button("ğŸ§ª æµ‹è¯•Embeddingé…ç½®", variant="primary", elem_classes=["primary-button"])

                    with gr.Column(scale=1):
                        # é…ç½®æµ‹è¯•æ—¥å¿—åŒºåŸŸ
                        gr.HTML("""
                        <div class="config-section">
                            <h3>ğŸ“‹ é…ç½®æµ‹è¯•æ—¥å¿—</h3>
                            <p>å®æ—¶æ˜¾ç¤ºæ¨¡å‹é…ç½®æµ‹è¯•çš„è¯¦ç»†è¿‡ç¨‹</p>
                        </div>
                        """)
                        config_log_output = gr.Textbox(
                            label="æµ‹è¯•æ—¥å¿—",
                            lines=20,
                            max_lines=25,
                            interactive=False,
                            value="",
                            placeholder="ğŸ’¡ ç‚¹å‡»æµ‹è¯•æŒ‰é’®æŸ¥çœ‹è¯¦ç»†æ—¥å¿—...\n\nğŸ” è¿™é‡Œä¼šæ˜¾ç¤ºï¼š\nâ€¢ è¿æ¥çŠ¶æ€\nâ€¢ è¯·æ±‚å‚æ•°\nâ€¢ å“åº”ç»“æœ\nâ€¢ é”™è¯¯è¯Šæ–­\nâ€¢ æ€§èƒ½æŒ‡æ ‡",
                            elem_classes=["log-container"]
                        )

                        # æ¸…ç©ºæ—¥å¿—æŒ‰é’®
                        btn_clear_config_log = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºæ—¥å¿—", variant="secondary")

                        # é…ç½®çŠ¶æ€å¡ç‰‡
                        gr.HTML("""
                        <div class="feature-card">
                            <h4>ğŸ“Š é…ç½®çŠ¶æ€</h4>
                            <div style="margin: 0.5rem 0;">
                                <div style="margin: 0.3rem 0;">
                                    <span class="status-indicator status-warning"></span>
                                    <span style="font-size: 0.9rem;">LLM: å¾…æµ‹è¯•</span>
                                </div>
                                <div style="margin: 0.3rem 0;">
                                    <span class="status-indicator status-warning"></span>
                                    <span style="font-size: 0.9rem;">Embedding: å¾…æµ‹è¯•</span>
                                </div>
                            </div>
                        </div>
                        """)

                        # æ¨èé…ç½®
                        gr.HTML("""
                        <div class="feature-card">
                            <h4>ğŸ’¡ æ¨èé…ç½®</h4>
                            <div style="font-size: 0.85rem; line-height: 1.5;">
                                <strong>ğŸš€ å¿«é€Ÿå¼€å§‹:</strong><br>
                                â€¢ OpenAI: gpt-4o-mini<br>
                                â€¢ Gemini: gemini-1.5-flash<br>
                                â€¢ DeepSeek: deepseek-chat<br><br>
                                <strong>âš™ï¸ å‚æ•°å»ºè®®:</strong><br>
                                â€¢ Temperature: 0.7-0.9<br>
                                â€¢ Max Tokens: 4096-8192<br>
                                â€¢ Timeout: 60-300ç§’
                            </div>
                        </div>
                        """)

            # Tab 3: å°è¯´å‚æ•°
            with gr.Tab("ğŸ“š å°è¯´å‚æ•°", id="params"):
                with gr.Column(elem_classes=["tab-content"]):
                    gr.Markdown("### ğŸ“– åŸºæœ¬è®¾ç½®")

                    topic_input = gr.Textbox(
                        label="ä¸»é¢˜ (Topic)",
                        lines=3,
                        placeholder="è¯·æè¿°å°è¯´çš„ä¸»é¢˜å’ŒèƒŒæ™¯...",
                        value=default_topic
                    )

                    with gr.Row():
                        genre_input = gr.Dropdown(
                            choices=genres,
                            label="ç±»å‹",
                            value=default_genre
                        )
                        num_chapters_input = gr.Number(
                            label="ç« èŠ‚æ•°",
                            value=default_num_chapters,
                            minimum=1
                        )
                        word_number_input = gr.Number(
                            label="æ¯ç« å­—æ•°",
                            value=default_word_number,
                            minimum=100
                        )

                    gr.Markdown("### ğŸ­ å¯é€‰å…ƒç´ ")

                    characters_involved_input = gr.Textbox(
                        label="æ ¸å¿ƒäººç‰©",
                        lines=2,
                        value=default_characters_involved,
                        placeholder="æè¿°ä¸»è¦è§’è‰²..."
                    )

                    with gr.Row():
                        key_items_input = gr.Textbox(
                            label="å…³é”®é“å…·",
                            value=default_key_items,
                            placeholder="é‡è¦ç‰©å“æˆ–é“å…·..."
                        )
                        scene_location_input = gr.Textbox(
                            label="ç©ºé—´åæ ‡",
                            value=default_scene_location,
                            placeholder="ä¸»è¦åœºæ™¯ä½ç½®..."
                        )
                        time_constraint_input = gr.Textbox(
                            label="æ—¶é—´å‹åŠ›",
                            value=default_time_constraint,
                            placeholder="æ—¶é—´ç›¸å…³çš„çº¦æŸ..."
                        )

            # Tab 4: æ–‡ä»¶ç®¡ç†
            with gr.Tab("ğŸ“ æ–‡ä»¶ç®¡ç†", id="files"):
                with gr.Column(elem_classes=["tab-content"]):
                    with gr.Tabs():
                        with gr.Tab("å°è¯´æ¶æ„"):
                            with gr.Row():
                                btn_load_architecture = gr.Button("åŠ è½½ Novel_architecture.txt")
                                btn_save_architecture = gr.Button("ä¿å­˜ä¿®æ”¹")
                        architecture_content = gr.Textbox(
                            label="å°è¯´æ¶æ„å†…å®¹",
                            lines=20,
                            placeholder="å°è¯´æ¶æ„å†…å®¹å°†æ˜¾ç¤ºåœ¨è¿™é‡Œ...",
                            interactive=True
                        )

                    with gr.Tab("ç« èŠ‚è“å›¾"):
                        with gr.Row():
                            btn_load_blueprint = gr.Button("åŠ è½½ Novel_directory.txt")
                            btn_save_blueprint = gr.Button("ä¿å­˜ä¿®æ”¹")
                        blueprint_content = gr.Textbox(
                            label="ç« èŠ‚è“å›¾å†…å®¹",
                            lines=20,
                            placeholder="ç« èŠ‚è“å›¾å†…å®¹å°†æ˜¾ç¤ºåœ¨è¿™é‡Œ...",
                            interactive=True
                        )

                    with gr.Tab("è§’è‰²çŠ¶æ€"):
                        with gr.Row():
                            btn_load_character = gr.Button("åŠ è½½ character_state.txt")
                            btn_save_character = gr.Button("ä¿å­˜ä¿®æ”¹")
                        character_content = gr.Textbox(
                            label="è§’è‰²çŠ¶æ€å†…å®¹",
                            lines=20,
                            placeholder="è§’è‰²çŠ¶æ€å†…å®¹å°†æ˜¾ç¤ºåœ¨è¿™é‡Œ...",
                            interactive=True
                        )

                    with gr.Tab("å…¨å±€æ‘˜è¦"):
                        with gr.Row():
                            btn_load_summary = gr.Button("åŠ è½½ global_summary.txt")
                            btn_save_summary = gr.Button("ä¿å­˜ä¿®æ”¹")
                        summary_content = gr.Textbox(
                            label="å…¨å±€æ‘˜è¦å†…å®¹",
                            lines=20,
                            placeholder="å…¨å±€æ‘˜è¦å†…å®¹å°†æ˜¾ç¤ºåœ¨è¿™é‡Œ...",
                            interactive=True
                        )

        # åœ¨Blocksä¸Šä¸‹æ–‡å†…ç›´æ¥è®¾ç½®äº‹ä»¶å¤„ç†å™¨
        # é…ç½®ç›¸å…³äº‹ä»¶
        btn_load_config.click(
            fn=handle_load_config,
            outputs=[
                llm_interface, llm_api_key, llm_base_url,
                llm_model, temperature, max_tokens, timeout,
                embedding_interface, embedding_api_key, embedding_base_url,
                embedding_model, retrieval_k,
                topic_input, genre_input, num_chapters_input,
                word_number_input, filepath_input, user_guidance_input,
                characters_involved_input, key_items_input,
                scene_location_input, time_constraint_input,
                log_output
            ]
        )

        btn_save_config.click(
            fn=handle_save_config,
            inputs=[
                llm_interface, llm_api_key, llm_base_url,
                llm_model, temperature, max_tokens, timeout,
                embedding_interface, embedding_api_key, embedding_base_url,
                embedding_model, retrieval_k,
                topic_input, genre_input, num_chapters_input,
                word_number_input, filepath_input, user_guidance_input,
                characters_involved_input, key_items_input,
                scene_location_input, time_constraint_input
            ],
            outputs=log_output
        )

        # æµ‹è¯•é…ç½®äº‹ä»¶
        btn_test_llm.click(
            fn=handle_test_llm_config,
            inputs=[
                llm_interface, llm_api_key, llm_base_url,
                llm_model, temperature, max_tokens, timeout, config_log_output
            ],
            outputs=config_log_output
        )

        btn_test_embedding.click(
            fn=handle_test_embedding_config,
            inputs=[
                embedding_interface, embedding_api_key,
                embedding_base_url, embedding_model, config_log_output
            ],
            outputs=config_log_output
        )

        # æ¸…ç©ºé…ç½®æ—¥å¿—äº‹ä»¶
        btn_clear_config_log.click(
            fn=handle_clear_config_log,
            outputs=config_log_output
        )

        # å¿«é€Ÿé…ç½®å’Œå¼€å§‹æŒ‰é’®äº‹ä»¶
        quick_config_btn.click(
            fn=lambda: None,
            js="() => { document.querySelector('[data-testid=\"tab-config\"]').click(); }"
        )

        quick_start_btn.click(
            fn=lambda: None,
            js="() => { document.querySelector('[data-testid=\"tab-main\"]').click(); }"
        )

        # æ ¸å¿ƒç”ŸæˆåŠŸèƒ½äº‹ä»¶
        btn_step1.click(
            fn=handle_generate_architecture,
            inputs=[
                llm_interface, llm_api_key, llm_base_url,
                llm_model, temperature, max_tokens, timeout,
                topic_input, genre_input, num_chapters_input,
                word_number_input, filepath_input, user_guidance_input,
                log_output
            ],
            outputs=log_output
        )

        btn_step2.click(
            fn=handle_generate_blueprint,
            inputs=[
                llm_interface, llm_api_key, llm_base_url,
                llm_model, temperature, max_tokens, timeout,
                filepath_input, log_output
            ],
            outputs=log_output
        )

        btn_step3.click(
            fn=handle_generate_chapter_draft,
            inputs=[
                llm_interface, llm_api_key, llm_base_url,
                llm_model, temperature, max_tokens, timeout,
                embedding_interface, embedding_api_key, embedding_base_url,
                embedding_model, retrieval_k,
                filepath_input, chapter_num_input, user_guidance_input,
                log_output
            ],
            outputs=[chapter_content, log_output]
        )

        btn_step4.click(
            fn=handle_finalize_chapter,
            inputs=[
                llm_interface, llm_api_key, llm_base_url,
                llm_model, temperature, max_tokens, timeout,
                embedding_interface, embedding_api_key, embedding_base_url,
                embedding_model,
                filepath_input, chapter_num_input, chapter_content,
                log_output
            ],
            outputs=log_output
        )

        # è¾…åŠ©åŠŸèƒ½äº‹ä»¶
        btn_consistency.click(
            fn=handle_consistency_check,
            inputs=[
                llm_interface, llm_api_key, llm_base_url,
                llm_model, temperature, max_tokens, timeout,
                filepath_input, chapter_num_input, log_output
            ],
            outputs=log_output
        )

        btn_import_knowledge.click(
            fn=handle_import_knowledge,
            inputs=[filepath_input, log_output],
            outputs=log_output
        )

        btn_clear_vectorstore.click(
            fn=handle_clear_vectorstore,
            inputs=[filepath_input, log_output],
            outputs=log_output
        )

        btn_plot_arcs.click(
            fn=handle_show_plot_arcs,
            inputs=[filepath_input, log_output],
            outputs=log_output
        )

        # æ–‡ä»¶ç®¡ç†äº‹ä»¶
        btn_load_architecture.click(
            fn=lambda filepath: handle_load_file(filepath, "Novel_architecture.txt"),
            inputs=filepath_input,
            outputs=[architecture_content, log_output]
        )

        btn_save_architecture.click(
            fn=lambda filepath, content: handle_save_file(filepath, "Novel_architecture.txt", content),
            inputs=[filepath_input, architecture_content],
            outputs=log_output
        )

        btn_load_blueprint.click(
            fn=lambda filepath: handle_load_file(filepath, "Novel_directory.txt"),
            inputs=filepath_input,
            outputs=[blueprint_content, log_output]
        )

        btn_save_blueprint.click(
            fn=lambda filepath, content: handle_save_file(filepath, "Novel_directory.txt", content),
            inputs=[filepath_input, blueprint_content],
            outputs=log_output
        )

        btn_load_character.click(
            fn=lambda filepath: handle_load_file(filepath, "character_state.txt"),
            inputs=filepath_input,
            outputs=[character_content, log_output]
        )

        btn_save_character.click(
            fn=lambda filepath, content: handle_save_file(filepath, "character_state.txt", content),
            inputs=[filepath_input, character_content],
            outputs=log_output
        )

        btn_load_summary.click(
            fn=lambda filepath: handle_load_file(filepath, "global_summary.txt"),
            inputs=filepath_input,
            outputs=[summary_content, log_output]
        )

        btn_save_summary.click(
            fn=lambda filepath, content: handle_save_file(filepath, "global_summary.txt", content),
            inputs=[filepath_input, summary_content],
            outputs=log_output
        )

        return demo

# äº‹ä»¶å¤„ç†å‡½æ•°
def handle_load_config():
    """å¤„ç†åŠ è½½é…ç½®äº‹ä»¶"""
    config_data, message = app.load_config_from_file()
    if config_data:
        return (
            config_data["llm_interface"],
            config_data["llm_api_key"],
            config_data["llm_base_url"],
            config_data["llm_model"],
            config_data["temperature"],
            config_data["max_tokens"],
            config_data["timeout"],
            config_data["embedding_interface"],
            config_data["embedding_api_key"],
            config_data["embedding_base_url"],
            config_data["embedding_model"],
            config_data["retrieval_k"],
            config_data["topic"],
            config_data["genre"],
            config_data["num_chapters"],
            config_data["word_number"],
            config_data["filepath"],
            config_data["user_guidance"],
            config_data["characters_involved"],
            config_data["key_items"],
            config_data["scene_location"],
            config_data["time_constraint"],
            message
        )
    else:
        return (
            "OpenAI", "", "https://api.openai.com/v1", "gpt-4o-mini", 0.7, 8192, 600,
            "OpenAI", "", "https://api.openai.com/v1", "text-embedding-ada-002", 4,
            "", "ç„å¹»", 10, 3000, "", "", "", "", "", "",
            message
        )

def handle_save_config(llm_interface, llm_api_key, llm_base_url, llm_model, temperature, max_tokens, timeout,
                      embedding_interface, embedding_api_key, embedding_base_url, embedding_model, retrieval_k,
                      topic, genre, num_chapters, word_number, filepath, user_guidance,
                      characters_involved, key_items, scene_location, time_constraint):
    """å¤„ç†ä¿å­˜é…ç½®äº‹ä»¶"""
    llm_config = app.get_llm_config_from_form(
        llm_interface, llm_api_key, llm_base_url, llm_model, temperature, max_tokens, timeout
    )
    embedding_config = app.get_embedding_config_from_form(
        embedding_interface, embedding_api_key, embedding_base_url, embedding_model, retrieval_k
    )
    novel_params = {
        "topic": topic,
        "genre": genre,
        "num_chapters": num_chapters,
        "word_number": word_number,
        "filepath": filepath,
        "user_guidance": user_guidance,
        "characters_involved": characters_involved,
        "key_items": key_items,
        "scene_location": scene_location,
        "time_constraint": time_constraint
    }

    return app.save_config_to_file(llm_config, embedding_config, novel_params)

def handle_test_llm_config(interface_format, api_key, base_url, model_name, temperature, max_tokens, timeout, current_log):
    """å¤„ç†æµ‹è¯•LLMé…ç½®äº‹ä»¶"""
    import datetime
    timestamp = datetime.datetime.now().strftime("%H:%M:%S")

    log_msg = current_log + f"\n[{timestamp}] ğŸ§ª å¼€å§‹æµ‹è¯•LLMé…ç½®...\n"
    log_msg += f"æ¥å£ç±»å‹: {interface_format}\n"
    log_msg += f"æ¨¡å‹åç§°: {model_name}\n"
    log_msg += f"Base URL: {base_url}\n"
    log_msg += f"Temperature: {temperature}\n"
    log_msg += f"Max Tokens: {max_tokens}\n"
    log_msg += f"Timeout: {timeout}ç§’\n"
    log_msg += "-" * 50 + "\n"

    try:
        log_msg += "æ­£åœ¨åˆ›å»ºLLMé€‚é…å™¨...\n"
        llm_adapter = create_llm_adapter(
            interface_format=interface_format,
            base_url=base_url,
            model_name=model_name,
            api_key=api_key,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout
        )
        log_msg += "âœ… LLMé€‚é…å™¨åˆ›å»ºæˆåŠŸ\n"

        test_prompt = "Please reply 'OK'"
        log_msg += f"å‘é€æµ‹è¯•æç¤º: {test_prompt}\n"
        log_msg += "ç­‰å¾…æ¨¡å‹å“åº”...\n"

        response = llm_adapter.invoke(test_prompt)
        if response and response.strip():
            log_msg += f"âœ… LLMé…ç½®æµ‹è¯•æˆåŠŸï¼\n"
            log_msg += f"æ¨¡å‹å›å¤: {response}\n"
            log_msg += f"å›å¤é•¿åº¦: {len(response)} å­—ç¬¦\n"
        else:
            log_msg += "âŒ LLMé…ç½®æµ‹è¯•å¤±è´¥ï¼šæœªè·å–åˆ°å“åº”\n"
            log_msg += "å¯èƒ½åŸå› ï¼š\n"
            log_msg += "1. APIå¯†é’¥æ— æ•ˆ\n"
            log_msg += "2. ç½‘ç»œè¿æ¥é—®é¢˜\n"
            log_msg += "3. æ¨¡å‹åç§°é”™è¯¯\n"
            log_msg += "4. æœåŠ¡å™¨æš‚æ—¶ä¸å¯ç”¨\n"

    except Exception as e:
        log_msg += f"âŒ LLMé…ç½®æµ‹è¯•å‡ºé”™: {str(e)}\n"
        log_msg += "è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n"
        import traceback
        log_msg += traceback.format_exc() + "\n"

    log_msg += "=" * 50 + "\n"
    return log_msg

def handle_test_embedding_config(interface_format, api_key, base_url, model_name, current_log):
    """å¤„ç†æµ‹è¯•Embeddingé…ç½®äº‹ä»¶"""
    import datetime
    timestamp = datetime.datetime.now().strftime("%H:%M:%S")

    log_msg = current_log + f"\n[{timestamp}] ğŸ” å¼€å§‹æµ‹è¯•Embeddingé…ç½®...\n"
    log_msg += f"æ¥å£ç±»å‹: {interface_format}\n"
    log_msg += f"æ¨¡å‹åç§°: {model_name}\n"
    log_msg += f"Base URL: {base_url}\n"
    log_msg += "-" * 50 + "\n"

    try:
        log_msg += "æ­£åœ¨åˆ›å»ºEmbeddingé€‚é…å™¨...\n"
        embedding_adapter = create_embedding_adapter(
            interface_format=interface_format,
            api_key=api_key,
            base_url=base_url,
            model_name=model_name
        )
        log_msg += "âœ… Embeddingé€‚é…å™¨åˆ›å»ºæˆåŠŸ\n"

        test_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬"
        log_msg += f"æµ‹è¯•æ–‡æœ¬: {test_text}\n"
        log_msg += "æ­£åœ¨ç”Ÿæˆå‘é‡...\n"

        embeddings = embedding_adapter.embed_query(test_text)
        if embeddings and len(embeddings) > 0:
            log_msg += f"âœ… Embeddingé…ç½®æµ‹è¯•æˆåŠŸï¼\n"
            log_msg += f"ç”Ÿæˆçš„å‘é‡ç»´åº¦: {len(embeddings)}\n"
            log_msg += f"å‘é‡å‰5ä¸ªå€¼: {embeddings[:5]}\n"
        else:
            log_msg += "âŒ Embeddingé…ç½®æµ‹è¯•å¤±è´¥ï¼šæœªè·å–åˆ°å‘é‡\n"
            log_msg += "å¯èƒ½åŸå› ï¼š\n"
            log_msg += "1. APIå¯†é’¥æ— æ•ˆ\n"
            log_msg += "2. æ¨¡å‹åç§°é”™è¯¯\n"
            log_msg += "3. ç½‘ç»œè¿æ¥é—®é¢˜\n"
            log_msg += "4. æœåŠ¡ä¸æ”¯æŒè¯¥æ¨¡å‹\n"

    except Exception as e:
        log_msg += f"âŒ Embeddingé…ç½®æµ‹è¯•å‡ºé”™: {str(e)}\n"
        log_msg += "è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n"
        import traceback
        log_msg += traceback.format_exc() + "\n"

    log_msg += "=" * 50 + "\n"
    return log_msg

def handle_clear_config_log():
    """æ¸…ç©ºé…ç½®æ—¥å¿—"""
    import datetime
    timestamp = datetime.datetime.now().strftime("%H:%M:%S")
    return f"[{timestamp}] ğŸ“‹ é…ç½®æµ‹è¯•æ—¥å¿—å·²æ¸…ç©º\n"

def handle_load_file(filepath, filename):
    """å¤„ç†æ–‡ä»¶åŠ è½½äº‹ä»¶"""
    if not filepath:
        return "", "âŒ è¯·å…ˆè®¾ç½®ä¿å­˜æ–‡ä»¶è·¯å¾„"

    full_path = os.path.join(filepath, filename)
    content = read_file(full_path)
    if content:
        return content, f"âœ… å·²åŠ è½½ {filename}"
    else:
        return "", f"âŒ æ— æ³•åŠ è½½ {filename}"

def handle_save_file(filepath, filename, content):
    """å¤„ç†æ–‡ä»¶ä¿å­˜äº‹ä»¶"""
    if not filepath:
        return "âŒ è¯·å…ˆè®¾ç½®ä¿å­˜æ–‡ä»¶è·¯å¾„"

    full_path = os.path.join(filepath, filename)
    try:
        clear_file_content(full_path)
        save_string_to_txt(content, full_path)
        return f"âœ… å·²ä¿å­˜ {filename}"
    except Exception as e:
        return f"âŒ ä¿å­˜ {filename} æ—¶å‡ºé”™: {str(e)}"

# æ ¸å¿ƒç”ŸæˆåŠŸèƒ½å¤„ç†å‡½æ•°
def handle_generate_architecture(llm_interface, llm_api_key, llm_base_url, llm_model, temperature, max_tokens, timeout,
                                topic, genre, num_chapters, word_number, filepath, user_guidance, current_log):
    """å¤„ç†ç”Ÿæˆå°è¯´æ¶æ„äº‹ä»¶"""
    if not filepath:
        return current_log + app.log_message("âŒ è¯·å…ˆè®¾ç½®ä¿å­˜æ–‡ä»¶è·¯å¾„")

    if not topic.strip():
        return current_log + app.log_message("âŒ è¯·å…ˆè¾“å…¥å°è¯´ä¸»é¢˜")

    try:
        log_msg = current_log + app.log_message("ğŸš€ å¼€å§‹ç”Ÿæˆå°è¯´æ¶æ„...")

        # åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œç”Ÿæˆ
        def generate_task():
            try:
                Novel_architecture_generate(
                    interface_format=llm_interface,
                    api_key=llm_api_key,
                    base_url=llm_base_url,
                    llm_model=llm_model,
                    topic=topic,
                    genre=genre,
                    number_of_chapters=int(num_chapters),
                    word_number=int(word_number),
                    filepath=filepath,
                    user_guidance=user_guidance,
                    temperature=temperature,
                    max_tokens=int(max_tokens),
                    timeout=int(timeout)
                )
                return "âœ… å°è¯´æ¶æ„ç”Ÿæˆå®Œæˆï¼"
            except Exception as e:
                return f"âŒ ç”Ÿæˆå°è¯´æ¶æ„æ—¶å‡ºé”™: {str(e)}"

        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥ä½¿ç”¨å¼‚æ­¥æˆ–è¿›åº¦æ¡
        result = generate_task()
        return log_msg + app.log_message(result)

    except Exception as e:
        return current_log + app.log_message(f"âŒ ç”Ÿæˆå°è¯´æ¶æ„æ—¶å‡ºé”™: {str(e)}")

def handle_generate_blueprint(llm_interface, llm_api_key, llm_base_url, llm_model, temperature, max_tokens, timeout,
                             filepath, current_log):
    """å¤„ç†ç”Ÿæˆç« èŠ‚è“å›¾äº‹ä»¶"""
    if not filepath:
        return current_log + app.log_message("âŒ è¯·å…ˆè®¾ç½®ä¿å­˜æ–‡ä»¶è·¯å¾„")

    try:
        log_msg = current_log + app.log_message("ğŸš€ å¼€å§‹ç”Ÿæˆç« èŠ‚è“å›¾...")

        def generate_task():
            try:
                Chapter_blueprint_generate(
                    interface_format=llm_interface,
                    api_key=llm_api_key,
                    base_url=llm_base_url,
                    llm_model=llm_model,
                    filepath=filepath,
                    temperature=temperature,
                    max_tokens=int(max_tokens),
                    timeout=int(timeout)
                )
                return "âœ… ç« èŠ‚è“å›¾ç”Ÿæˆå®Œæˆï¼"
            except Exception as e:
                return f"âŒ ç”Ÿæˆç« èŠ‚è“å›¾æ—¶å‡ºé”™: {str(e)}"

        result = generate_task()
        return log_msg + app.log_message(result)

    except Exception as e:
        return current_log + app.log_message(f"âŒ ç”Ÿæˆç« èŠ‚è“å›¾æ—¶å‡ºé”™: {str(e)}")

def handle_generate_chapter_draft(llm_interface, llm_api_key, llm_base_url, llm_model, temperature, max_tokens, timeout,
                                 embedding_interface, embedding_api_key, embedding_base_url, embedding_model, retrieval_k,
                                 filepath, chapter_num, user_guidance, current_log):
    """å¤„ç†ç”Ÿæˆç« èŠ‚è‰ç¨¿äº‹ä»¶"""
    if not filepath:
        return "", current_log + app.log_message("âŒ è¯·å…ˆè®¾ç½®ä¿å­˜æ–‡ä»¶è·¯å¾„")

    try:
        log_msg = current_log + app.log_message(f"ğŸš€ å¼€å§‹ç”Ÿæˆç¬¬{chapter_num}ç« è‰ç¨¿...")

        def generate_task():
            try:
                result = generate_chapter_draft(
                    interface_format=llm_interface,
                    api_key=llm_api_key,
                    base_url=llm_base_url,
                    llm_model=llm_model,
                    embedding_interface_format=embedding_interface,
                    embedding_api_key=embedding_api_key,
                    embedding_base_url=embedding_base_url,
                    embedding_model_name=embedding_model,
                    retrieval_k=int(retrieval_k),
                    filepath=filepath,
                    chapter_num=int(chapter_num),
                    user_guidance=user_guidance,
                    temperature=temperature,
                    max_tokens=int(max_tokens),
                    timeout=int(timeout)
                )

                # è¯»å–ç”Ÿæˆçš„ç« èŠ‚å†…å®¹
                chapter_file = os.path.join(filepath, f"chapter_{chapter_num}.txt")
                chapter_content = read_file(chapter_file)

                return chapter_content, "âœ… ç« èŠ‚è‰ç¨¿ç”Ÿæˆå®Œæˆï¼"
            except Exception as e:
                return "", f"âŒ ç”Ÿæˆç« èŠ‚è‰ç¨¿æ—¶å‡ºé”™: {str(e)}"

        chapter_content, result_msg = generate_task()
        return chapter_content, log_msg + app.log_message(result_msg)

    except Exception as e:
        return "", current_log + app.log_message(f"âŒ ç”Ÿæˆç« èŠ‚è‰ç¨¿æ—¶å‡ºé”™: {str(e)}")

def handle_finalize_chapter(llm_interface, llm_api_key, llm_base_url, llm_model, temperature, max_tokens, timeout,
                           embedding_interface, embedding_api_key, embedding_base_url, embedding_model,
                           filepath, chapter_num, chapter_content, current_log):
    """å¤„ç†å®šç¨¿ç« èŠ‚äº‹ä»¶"""
    if not filepath:
        return current_log + app.log_message("âŒ è¯·å…ˆè®¾ç½®ä¿å­˜æ–‡ä»¶è·¯å¾„")

    if not chapter_content.strip():
        return current_log + app.log_message("âŒ ç« èŠ‚å†…å®¹ä¸ºç©ºï¼Œæ— æ³•å®šç¨¿")

    try:
        log_msg = current_log + app.log_message(f"ğŸš€ å¼€å§‹å®šç¨¿ç¬¬{chapter_num}ç« ...")

        # å…ˆä¿å­˜å½“å‰ç« èŠ‚å†…å®¹
        chapter_file = os.path.join(filepath, f"chapter_{chapter_num}.txt")
        save_string_to_txt(chapter_content, chapter_file)

        def finalize_task():
            try:
                finalize_chapter(
                    interface_format=llm_interface,
                    api_key=llm_api_key,
                    base_url=llm_base_url,
                    llm_model=llm_model,
                    embedding_interface_format=embedding_interface,
                    embedding_api_key=embedding_api_key,
                    embedding_base_url=embedding_base_url,
                    embedding_model_name=embedding_model,
                    filepath=filepath,
                    chapter_num=int(chapter_num),
                    temperature=temperature,
                    max_tokens=int(max_tokens),
                    timeout=int(timeout)
                )
                return "âœ… ç« èŠ‚å®šç¨¿å®Œæˆï¼å·²æ›´æ–°å…¨å±€æ‘˜è¦ã€è§’è‰²çŠ¶æ€å’Œå‘é‡åº“ã€‚"
            except Exception as e:
                return f"âŒ å®šç¨¿ç« èŠ‚æ—¶å‡ºé”™: {str(e)}"

        result = finalize_task()
        return log_msg + app.log_message(result)

    except Exception as e:
        return current_log + app.log_message(f"âŒ å®šç¨¿ç« èŠ‚æ—¶å‡ºé”™: {str(e)}")

# è¾…åŠ©åŠŸèƒ½å¤„ç†å‡½æ•°
def handle_consistency_check(llm_interface, llm_api_key, llm_base_url, llm_model, temperature, max_tokens, timeout,
                            filepath, chapter_num, current_log):
    """å¤„ç†ä¸€è‡´æ€§æ£€æŸ¥äº‹ä»¶"""
    if not filepath:
        return current_log + app.log_message("âŒ è¯·å…ˆè®¾ç½®ä¿å­˜æ–‡ä»¶è·¯å¾„")

    try:
        log_msg = current_log + app.log_message(f"ğŸ” å¼€å§‹æ£€æŸ¥ç¬¬{chapter_num}ç« çš„ä¸€è‡´æ€§...")

        def check_task():
            try:
                result = check_consistency(
                    interface_format=llm_interface,
                    api_key=llm_api_key,
                    base_url=llm_base_url,
                    llm_model=llm_model,
                    filepath=filepath,
                    chapter_num=int(chapter_num),
                    temperature=temperature,
                    max_tokens=int(max_tokens),
                    timeout=int(timeout)
                )
                return f"âœ… ä¸€è‡´æ€§æ£€æŸ¥å®Œæˆï¼\n{result}"
            except Exception as e:
                return f"âŒ ä¸€è‡´æ€§æ£€æŸ¥æ—¶å‡ºé”™: {str(e)}"

        result = check_task()
        return log_msg + app.log_message(result)

    except Exception as e:
        return current_log + app.log_message(f"âŒ ä¸€è‡´æ€§æ£€æŸ¥æ—¶å‡ºé”™: {str(e)}")

def handle_import_knowledge(filepath, current_log):
    """å¤„ç†å¯¼å…¥çŸ¥è¯†åº“äº‹ä»¶"""
    if not filepath:
        return current_log + app.log_message("âŒ è¯·å…ˆè®¾ç½®ä¿å­˜æ–‡ä»¶è·¯å¾„")

    try:
        log_msg = current_log + app.log_message("ğŸ“š å¼€å§‹å¯¼å…¥çŸ¥è¯†åº“...")

        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥æä¾›æ–‡ä»¶é€‰æ‹©ç•Œé¢
        knowledge_file = os.path.join(filepath, "knowledge.txt")
        if os.path.exists(knowledge_file):
            import_knowledge_file(knowledge_file, filepath)
            return log_msg + app.log_message("âœ… çŸ¥è¯†åº“å¯¼å…¥å®Œæˆï¼")
        else:
            return log_msg + app.log_message(f"âŒ æœªæ‰¾åˆ°çŸ¥è¯†åº“æ–‡ä»¶: {knowledge_file}")

    except Exception as e:
        return current_log + app.log_message(f"âŒ å¯¼å…¥çŸ¥è¯†åº“æ—¶å‡ºé”™: {str(e)}")

def handle_clear_vectorstore(filepath, current_log):
    """å¤„ç†æ¸…ç©ºå‘é‡åº“äº‹ä»¶"""
    if not filepath:
        return current_log + app.log_message("âŒ è¯·å…ˆè®¾ç½®ä¿å­˜æ–‡ä»¶è·¯å¾„")

    try:
        log_msg = current_log + app.log_message("ğŸ—‘ï¸ å¼€å§‹æ¸…ç©ºå‘é‡åº“...")
        clear_vector_store(filepath)
        return log_msg + app.log_message("âœ… å‘é‡åº“å·²æ¸…ç©ºï¼")

    except Exception as e:
        return current_log + app.log_message(f"âŒ æ¸…ç©ºå‘é‡åº“æ—¶å‡ºé”™: {str(e)}")

def handle_show_plot_arcs(filepath, current_log):
    """å¤„ç†æŸ¥çœ‹å‰§æƒ…è¦ç‚¹äº‹ä»¶"""
    if not filepath:
        return current_log + app.log_message("âŒ è¯·å…ˆè®¾ç½®ä¿å­˜æ–‡ä»¶è·¯å¾„")

    try:
        plot_arcs_file = os.path.join(filepath, "plot_arcs.txt")
        content = read_file(plot_arcs_file)
        if content:
            return current_log + app.log_message(f"ğŸ“– å‰§æƒ…è¦ç‚¹å†…å®¹ï¼š\n{content}")
        else:
            return current_log + app.log_message("âŒ æœªæ‰¾åˆ°å‰§æƒ…è¦ç‚¹æ–‡ä»¶")

    except Exception as e:
        return current_log + app.log_message(f"âŒ æŸ¥çœ‹å‰§æƒ…è¦ç‚¹æ—¶å‡ºé”™: {str(e)}")

# åˆ é™¤é‡å¤çš„äº‹ä»¶å¤„ç†å™¨å‡½æ•°ï¼Œå·²ç»åœ¨create_interfaceä¸­ç›´æ¥è®¾ç½®

if __name__ == "__main__":
    demo = create_interface()

    print("ğŸš€ å¯åŠ¨AIå°è¯´ç”Ÿæˆå™¨Webç•Œé¢...")
    print("ğŸ“ è®¿é—®åœ°å€: http://localhost:7860")

    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True
    )