# web_app.py
# -*- coding: utf-8 -*-
"""
AI_NovelGenerator Gradio Web Interface
åŸºäºåŸæœ‰GUIåŠŸèƒ½åˆ›å»ºçš„Webç•Œé¢ç‰ˆæœ¬
"""

import gradio as gr
import os
import json
import threading
import traceback
from typing import Optional, Tuple, Dict, Any

# å¯¼å…¥åŸæœ‰çš„æ ¸å¿ƒåŠŸèƒ½æ¨¡å—
from config_manager import load_config, save_config, test_llm_config, test_embedding_config
from novel_generator import (
    Novel_architecture_generate,
    Chapter_blueprint_generate,
    generate_chapter_draft,
    finalize_chapter,
    import_knowledge_file,
    clear_vector_store
)
from consistency_checker import check_consistency
from utils import read_file, save_string_to_txt, clear_file_content
from llm_adapters import create_llm_adapter
from embedding_adapters import create_embedding_adapter

class NovelGeneratorWebApp:
    """AIå°è¯´ç”Ÿæˆå™¨Webåº”ç”¨ç±»"""

    def __init__(self):
        self.config_file = "config.json"
        self.loaded_config = load_config(self.config_file)

        # åˆå§‹åŒ–é»˜è®¤é…ç½®
        self.init_default_config()

        # çŠ¶æ€å˜é‡
        self.current_chapter_num = 1
        self.generation_in_progress = False

    def init_default_config(self):
        """åˆå§‹åŒ–é»˜è®¤é…ç½®"""
        if self.loaded_config:
            self.last_llm = self.loaded_config.get("last_interface_format", "OpenAI")
            self.last_embedding = self.loaded_config.get("last_embedding_interface_format", "OpenAI")
        else:
            self.last_llm = "OpenAI"
            self.last_embedding = "OpenAI"

    def log_message(self, message: str) -> str:
        """æ·»åŠ æ—¥å¿—æ¶ˆæ¯"""
        import datetime
        timestamp = datetime.datetime.now().strftime("%H:%M:%S")
        return f"[{timestamp}] {message}\n"

    def get_llm_config_from_form(self, interface_format, api_key, base_url, model_name,
                                temperature, max_tokens, timeout):
        """ä»è¡¨å•è·å–LLMé…ç½®"""
        return {
            "interface_format": interface_format,
            "api_key": api_key,
            "base_url": base_url,
            "model_name": model_name,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "timeout": timeout
        }

    def get_embedding_config_from_form(self, interface_format, api_key, base_url,
                                     model_name, retrieval_k):
        """ä»è¡¨å•è·å–Embeddingé…ç½®"""
        return {
            "interface_format": interface_format,
            "api_key": api_key,
            "base_url": base_url,
            "model_name": model_name,
            "retrieval_k": retrieval_k
        }

    def save_config_to_file(self, llm_config, embedding_config, novel_params):
        """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
        try:
            config_data = {
                "last_interface_format": llm_config["interface_format"],
                "last_embedding_interface_format": embedding_config["interface_format"],
                "llm_configs": {
                    llm_config["interface_format"]: {
                        "api_key": llm_config["api_key"],
                        "base_url": llm_config["base_url"],
                        "model_name": llm_config["model_name"],
                        "temperature": llm_config["temperature"],
                        "max_tokens": llm_config["max_tokens"],
                        "timeout": llm_config["timeout"]
                    }
                },
                "embedding_configs": {
                    embedding_config["interface_format"]: {
                        "api_key": embedding_config["api_key"],
                        "base_url": embedding_config["base_url"],
                        "model_name": embedding_config["model_name"],
                        "retrieval_k": embedding_config["retrieval_k"]
                    }
                },
                "other_params": novel_params
            }

            success = save_config(config_data, self.config_file)
            if success:
                self.loaded_config = config_data
                return "âœ… é…ç½®ä¿å­˜æˆåŠŸï¼"
            else:
                return "âŒ é…ç½®ä¿å­˜å¤±è´¥ï¼"
        except Exception as e:
            return f"âŒ ä¿å­˜é…ç½®æ—¶å‡ºé”™: {str(e)}"

    def load_config_from_file(self):
        """ä»æ–‡ä»¶åŠ è½½é…ç½®"""
        try:
            cfg = load_config(self.config_file)
            if not cfg:
                return None, "æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶"

            # æå–LLMé…ç½®
            last_llm = cfg.get("last_interface_format", "OpenAI")
            llm_configs = cfg.get("llm_configs", {})
            llm_config = llm_configs.get(last_llm, {})

            # æå–Embeddingé…ç½®
            last_embedding = cfg.get("last_embedding_interface_format", "OpenAI")
            embedding_configs = cfg.get("embedding_configs", {})
            embedding_config = embedding_configs.get(last_embedding, {})

            # æå–å…¶ä»–å‚æ•°
            other_params = cfg.get("other_params", {})

            return {
                "llm_interface": last_llm,
                "llm_api_key": llm_config.get("api_key", ""),
                "llm_base_url": llm_config.get("base_url", "https://api.openai.com/v1"),
                "llm_model": llm_config.get("model_name", "gpt-4o-mini"),
                "temperature": llm_config.get("temperature", 0.7),
                "max_tokens": llm_config.get("max_tokens", 8192),
                "timeout": llm_config.get("timeout", 600),
                "embedding_interface": last_embedding,
                "embedding_api_key": embedding_config.get("api_key", ""),
                "embedding_base_url": embedding_config.get("base_url", "https://api.openai.com/v1"),
                "embedding_model": embedding_config.get("model_name", "text-embedding-ada-002"),
                "retrieval_k": embedding_config.get("retrieval_k", 4),
                "topic": other_params.get("topic", ""),
                "genre": other_params.get("genre", "ç„å¹»"),
                "num_chapters": other_params.get("num_chapters", 10),
                "word_number": other_params.get("word_number", 3000),
                "filepath": other_params.get("filepath", ""),
                "user_guidance": other_params.get("user_guidance", ""),
                "characters_involved": other_params.get("characters_involved", ""),
                "key_items": other_params.get("key_items", ""),
                "scene_location": other_params.get("scene_location", ""),
                "time_constraint": other_params.get("time_constraint", "")
            }, "âœ… é…ç½®åŠ è½½æˆåŠŸï¼"
        except Exception as e:
            return None, f"âŒ åŠ è½½é…ç½®æ—¶å‡ºé”™: {str(e)}"

# åˆ›å»ºå…¨å±€åº”ç”¨å®ä¾‹
app = NovelGeneratorWebApp()

def create_interface():
    """åˆ›å»ºGradioç•Œé¢"""

    # å®šä¹‰LLMæ¥å£é€‰é¡¹
    llm_interfaces = ["OpenAI", "DeepSeek", "Azure OpenAI", "Azure AI", "Ollama",
                     "ML Studio", "Gemini", "é˜¿é‡Œäº‘ç™¾ç‚¼", "ç«å±±å¼•æ“", "ç¡…åŸºæµåŠ¨"]

    # å®šä¹‰ç±»å‹é€‰é¡¹
    genres = ["ç„å¹»", "ä»™ä¾ ", "éƒ½å¸‚", "å†å²", "ç§‘å¹»", "å†›äº‹", "æ¸¸æˆ", "ä½“è‚²", "æ‚¬ç–‘", "å…¶ä»–"]

    # è‡ªåŠ¨åŠ è½½å·²ä¿å­˜çš„é…ç½®
    config_data, config_message = app.load_config_from_file()
    if config_data:
        print(f"âœ… è‡ªåŠ¨åŠ è½½é…ç½®æˆåŠŸ: {config_message}")
        # ä½¿ç”¨åŠ è½½çš„é…ç½®ä½œä¸ºé»˜è®¤å€¼
        default_llm_interface = config_data["llm_interface"]
        default_llm_api_key = config_data["llm_api_key"]
        default_llm_base_url = config_data["llm_base_url"]
        default_llm_model = config_data["llm_model"]
        default_temperature = config_data["temperature"]
        default_max_tokens = config_data["max_tokens"]
        default_timeout = config_data["timeout"]
        default_embedding_interface = config_data["embedding_interface"]
        default_embedding_api_key = config_data["embedding_api_key"]
        default_embedding_base_url = config_data["embedding_base_url"]
        default_embedding_model = config_data["embedding_model"]
        default_retrieval_k = config_data["retrieval_k"]
        default_topic = config_data["topic"]
        default_genre = config_data["genre"]
        default_num_chapters = config_data["num_chapters"]
        default_word_number = config_data["word_number"]
        default_filepath = config_data["filepath"]
        default_user_guidance = config_data["user_guidance"]
        default_characters_involved = config_data["characters_involved"]
        default_key_items = config_data["key_items"]
        default_scene_location = config_data["scene_location"]
        default_time_constraint = config_data["time_constraint"]
    else:
        print(f"âš ï¸ æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤å€¼: {config_message}")
        # ä½¿ç”¨é»˜è®¤å€¼
        default_llm_interface = "OpenAI"
        default_llm_api_key = ""
        default_llm_base_url = "https://api.openai.com/v1"
        default_llm_model = "gpt-4o-mini"
        default_temperature = 0.7
        default_max_tokens = 8192
        default_timeout = 600
        default_embedding_interface = "OpenAI"
        default_embedding_api_key = ""
        default_embedding_base_url = "https://api.openai.com/v1"
        default_embedding_model = "text-embedding-ada-002"
        default_retrieval_k = 4
        default_topic = ""
        default_genre = "ç„å¹»"
        default_num_chapters = 10
        default_word_number = 3000
        default_filepath = ""
        default_user_guidance = ""
        default_characters_involved = ""
        default_key_items = ""
        default_scene_location = ""
        default_time_constraint = ""

    with gr.Blocks(title="AIå°è¯´ç”Ÿæˆå™¨", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# ğŸ¯ AIå°è¯´ç”Ÿæˆå™¨ Webç‰ˆ")
        gr.Markdown("åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½å°è¯´åˆ›ä½œå·¥å…·")

        # ä¸»è¦çŠ¶æ€å˜é‡
        log_state = gr.State("")

        with gr.Tabs() as tabs:
            # Tab 1: ä¸»è¦åŠŸèƒ½
            with gr.Tab("ğŸ“ ä¸»è¦åŠŸèƒ½", id="main"):
                with gr.Row():
                    with gr.Column(scale=2):
                        # å·¦ä¾§ï¼šç« èŠ‚å†…å®¹å’Œæ“ä½œæŒ‰é’®
                        gr.Markdown("### ğŸ“– å½“å‰ç« èŠ‚å†…å®¹")
                        chapter_content = gr.Textbox(
                            label="ç« èŠ‚å†…å®¹ï¼ˆå¯ç¼–è¾‘ï¼‰",
                            lines=15,
                            max_lines=20,
                            placeholder="ç”Ÿæˆçš„ç« èŠ‚å†…å®¹å°†æ˜¾ç¤ºåœ¨è¿™é‡Œ...",
                            interactive=True
                        )

                        # StepæŒ‰é’®åŒºåŸŸ
                        gr.Markdown("### ğŸš€ ç”Ÿæˆæµç¨‹")
                        with gr.Row():
                            btn_step1 = gr.Button("Step1. ç”Ÿæˆæ¶æ„", variant="primary")
                            btn_step2 = gr.Button("Step2. ç”Ÿæˆç›®å½•", variant="primary")
                            btn_step3 = gr.Button("Step3. ç”Ÿæˆè‰ç¨¿", variant="primary")
                            btn_step4 = gr.Button("Step4. å®šç¨¿ç« èŠ‚", variant="primary")

                        # è¾…åŠ©åŠŸèƒ½æŒ‰é’®
                        gr.Markdown("### ğŸ”§ è¾…åŠ©åŠŸèƒ½")
                        with gr.Row():
                            btn_consistency = gr.Button("ä¸€è‡´æ€§æ£€æŸ¥")
                            btn_import_knowledge = gr.Button("å¯¼å…¥çŸ¥è¯†åº“")
                            btn_clear_vectorstore = gr.Button("æ¸…ç©ºå‘é‡åº“", variant="stop")
                            btn_plot_arcs = gr.Button("æŸ¥çœ‹å‰§æƒ…è¦ç‚¹")

                        # æ—¥å¿—åŒºåŸŸ
                        gr.Markdown("### ğŸ“‹ è¾“å‡ºæ—¥å¿—")
                        log_output = gr.Textbox(
                            label="ç³»ç»Ÿæ—¥å¿—",
                            lines=8,
                            max_lines=10,
                            interactive=False,
                            value=""
                        )

                    with gr.Column(scale=1):
                        # å³ä¾§ï¼šé…ç½®å’Œå‚æ•°
                        gr.Markdown("### âš™ï¸ å¿«é€Ÿé…ç½®")

                        # æ–‡ä»¶è·¯å¾„è®¾ç½®
                        filepath_input = gr.Textbox(
                            label="ä¿å­˜è·¯å¾„",
                            placeholder="è¯·è¾“å…¥å°è¯´æ–‡ä»¶ä¿å­˜è·¯å¾„",
                            value=default_filepath
                        )

                        # ç« èŠ‚å·è®¾ç½®
                        chapter_num_input = gr.Number(
                            label="å½“å‰ç« èŠ‚å·",
                            value=1,
                            minimum=1,
                            step=1
                        )

                        # æœ¬ç« æŒ‡å¯¼
                        user_guidance_input = gr.Textbox(
                            label="æœ¬ç« æŒ‡å¯¼",
                            lines=3,
                            value=default_user_guidance,
                            placeholder="å¯¹æœ¬ç« å‰§æƒ…çš„æœŸæœ›æˆ–æç¤º..."
                        )

                        # é…ç½®åŠ è½½/ä¿å­˜æŒ‰é’®
                        with gr.Row():
                            btn_load_config = gr.Button("åŠ è½½é…ç½®")
                            btn_save_config = gr.Button("ä¿å­˜é…ç½®")

            # Tab 2: è¯¦ç»†é…ç½®
            with gr.Tab("ğŸ”§ æ¨¡å‹é…ç½®", id="config"):
                with gr.Row():
                    with gr.Column(scale=2):
                        with gr.Row():
                            with gr.Column():
                                gr.Markdown("### ğŸ¤– LLMæ¨¡å‹è®¾ç½®")
                                llm_interface = gr.Dropdown(
                                    choices=llm_interfaces,
                                    label="æ¥å£ç±»å‹",
                                    value=default_llm_interface
                                )
                                llm_api_key = gr.Textbox(
                                    label="API Key",
                                    type="password",
                                    value=default_llm_api_key,
                                    placeholder="è¯·è¾“å…¥APIå¯†é’¥"
                                )
                                llm_base_url = gr.Textbox(
                                    label="Base URL",
                                    value=default_llm_base_url,
                                    placeholder="APIåŸºç¡€URL"
                                )
                                llm_model = gr.Textbox(
                                    label="æ¨¡å‹åç§°",
                                    value=default_llm_model,
                                    placeholder="æ¨¡å‹åç§°"
                                )

                                with gr.Row():
                                    temperature = gr.Slider(
                                        label="Temperature",
                                        minimum=0.0,
                                        maximum=2.0,
                                        value=default_temperature,
                                        step=0.1
                                    )
                                    max_tokens = gr.Number(
                                        label="Max Tokens",
                                        value=default_max_tokens,
                                        minimum=1
                                    )
                                    timeout = gr.Number(
                                        label="Timeout (ç§’)",
                                        value=default_timeout,
                                        minimum=1
                                    )

                                btn_test_llm = gr.Button("æµ‹è¯•LLMé…ç½®", variant="secondary")

                            with gr.Column():
                                gr.Markdown("### ğŸ” Embeddingæ¨¡å‹è®¾ç½®")
                                embedding_interface = gr.Dropdown(
                                    choices=llm_interfaces,
                                    label="æ¥å£ç±»å‹",
                                    value=default_embedding_interface
                                )
                                embedding_api_key = gr.Textbox(
                                    label="API Key",
                                    type="password",
                                    value=default_embedding_api_key,
                                    placeholder="è¯·è¾“å…¥APIå¯†é’¥"
                                )
                                embedding_base_url = gr.Textbox(
                                    label="Base URL",
                                    value=default_embedding_base_url,
                                    placeholder="APIåŸºç¡€URL"
                                )
                                embedding_model = gr.Textbox(
                                    label="æ¨¡å‹åç§°",
                                    value=default_embedding_model,
                                    placeholder="Embeddingæ¨¡å‹åç§°"
                                )
                                retrieval_k = gr.Number(
                                    label="æ£€ç´¢æ•°é‡ (K)",
                                    value=default_retrieval_k,
                                    minimum=1,
                                    maximum=20
                                )

                                btn_test_embedding = gr.Button("æµ‹è¯•Embeddingé…ç½®", variant="secondary")

                    with gr.Column(scale=1):
                        # é…ç½®æµ‹è¯•æ—¥å¿—åŒºåŸŸ
                        gr.Markdown("### ğŸ“‹ é…ç½®æµ‹è¯•æ—¥å¿—")
                        config_log_output = gr.Textbox(
                            label="æµ‹è¯•æ—¥å¿—",
                            lines=20,
                            max_lines=25,
                            interactive=False,
                            value="",
                            placeholder="ç‚¹å‡»æµ‹è¯•æŒ‰é’®æŸ¥çœ‹è¯¦ç»†æ—¥å¿—..."
                        )

                        # æ¸…ç©ºæ—¥å¿—æŒ‰é’®
                        btn_clear_config_log = gr.Button("æ¸…ç©ºæ—¥å¿—", variant="secondary")

            # Tab 3: å°è¯´å‚æ•°
            with gr.Tab("ğŸ“š å°è¯´å‚æ•°", id="params"):
                with gr.Column():
                    gr.Markdown("### ğŸ“– åŸºæœ¬è®¾ç½®")

                    topic_input = gr.Textbox(
                        label="ä¸»é¢˜ (Topic)",
                        lines=3,
                        placeholder="è¯·æè¿°å°è¯´çš„ä¸»é¢˜å’ŒèƒŒæ™¯...",
                        value=default_topic
                    )

                    with gr.Row():
                        genre_input = gr.Dropdown(
                            choices=genres,
                            label="ç±»å‹",
                            value=default_genre
                        )
                        num_chapters_input = gr.Number(
                            label="ç« èŠ‚æ•°",
                            value=default_num_chapters,
                            minimum=1
                        )
                        word_number_input = gr.Number(
                            label="æ¯ç« å­—æ•°",
                            value=default_word_number,
                            minimum=100
                        )

                    gr.Markdown("### ğŸ­ å¯é€‰å…ƒç´ ")

                    characters_involved_input = gr.Textbox(
                        label="æ ¸å¿ƒäººç‰©",
                        lines=2,
                        value=default_characters_involved,
                        placeholder="æè¿°ä¸»è¦è§’è‰²..."
                    )

                    with gr.Row():
                        key_items_input = gr.Textbox(
                            label="å…³é”®é“å…·",
                            value=default_key_items,
                            placeholder="é‡è¦ç‰©å“æˆ–é“å…·..."
                        )
                        scene_location_input = gr.Textbox(
                            label="ç©ºé—´åæ ‡",
                            value=default_scene_location,
                            placeholder="ä¸»è¦åœºæ™¯ä½ç½®..."
                        )
                        time_constraint_input = gr.Textbox(
                            label="æ—¶é—´å‹åŠ›",
                            value=default_time_constraint,
                            placeholder="æ—¶é—´ç›¸å…³çš„çº¦æŸ..."
                        )

            # Tab 4: æ–‡ä»¶ç®¡ç†
            with gr.Tab("ğŸ“ æ–‡ä»¶ç®¡ç†", id="files"):
                with gr.Tabs():
                    with gr.Tab("å°è¯´æ¶æ„"):
                        with gr.Row():
                            btn_load_architecture = gr.Button("åŠ è½½ Novel_architecture.txt")
                            btn_save_architecture = gr.Button("ä¿å­˜ä¿®æ”¹")
                        architecture_content = gr.Textbox(
                            label="å°è¯´æ¶æ„å†…å®¹",
                            lines=20,
                            placeholder="å°è¯´æ¶æ„å†…å®¹å°†æ˜¾ç¤ºåœ¨è¿™é‡Œ...",
                            interactive=True
                        )

                    with gr.Tab("ç« èŠ‚è“å›¾"):
                        with gr.Row():
                            btn_load_blueprint = gr.Button("åŠ è½½ Novel_directory.txt")
                            btn_save_blueprint = gr.Button("ä¿å­˜ä¿®æ”¹")
                        blueprint_content = gr.Textbox(
                            label="ç« èŠ‚è“å›¾å†…å®¹",
                            lines=20,
                            placeholder="ç« èŠ‚è“å›¾å†…å®¹å°†æ˜¾ç¤ºåœ¨è¿™é‡Œ...",
                            interactive=True
                        )

                    with gr.Tab("è§’è‰²çŠ¶æ€"):
                        with gr.Row():
                            btn_load_character = gr.Button("åŠ è½½ character_state.txt")
                            btn_save_character = gr.Button("ä¿å­˜ä¿®æ”¹")
                        character_content = gr.Textbox(
                            label="è§’è‰²çŠ¶æ€å†…å®¹",
                            lines=20,
                            placeholder="è§’è‰²çŠ¶æ€å†…å®¹å°†æ˜¾ç¤ºåœ¨è¿™é‡Œ...",
                            interactive=True
                        )

                    with gr.Tab("å…¨å±€æ‘˜è¦"):
                        with gr.Row():
                            btn_load_summary = gr.Button("åŠ è½½ global_summary.txt")
                            btn_save_summary = gr.Button("ä¿å­˜ä¿®æ”¹")
                        summary_content = gr.Textbox(
                            label="å…¨å±€æ‘˜è¦å†…å®¹",
                            lines=20,
                            placeholder="å…¨å±€æ‘˜è¦å†…å®¹å°†æ˜¾ç¤ºåœ¨è¿™é‡Œ...",
                            interactive=True
                        )

        # åœ¨Blocksä¸Šä¸‹æ–‡å†…ç›´æ¥è®¾ç½®äº‹ä»¶å¤„ç†å™¨
        # é…ç½®ç›¸å…³äº‹ä»¶
        btn_load_config.click(
            fn=handle_load_config,
            outputs=[
                llm_interface, llm_api_key, llm_base_url,
                llm_model, temperature, max_tokens, timeout,
                embedding_interface, embedding_api_key, embedding_base_url,
                embedding_model, retrieval_k,
                topic_input, genre_input, num_chapters_input,
                word_number_input, filepath_input, user_guidance_input,
                characters_involved_input, key_items_input,
                scene_location_input, time_constraint_input,
                log_output
            ]
        )

        btn_save_config.click(
            fn=handle_save_config,
            inputs=[
                llm_interface, llm_api_key, llm_base_url,
                llm_model, temperature, max_tokens, timeout,
                embedding_interface, embedding_api_key, embedding_base_url,
                embedding_model, retrieval_k,
                topic_input, genre_input, num_chapters_input,
                word_number_input, filepath_input, user_guidance_input,
                characters_involved_input, key_items_input,
                scene_location_input, time_constraint_input
            ],
            outputs=log_output
        )

        # æµ‹è¯•é…ç½®äº‹ä»¶
        btn_test_llm.click(
            fn=handle_test_llm_config,
            inputs=[
                llm_interface, llm_api_key, llm_base_url,
                llm_model, temperature, max_tokens, timeout, config_log_output
            ],
            outputs=config_log_output
        )

        btn_test_embedding.click(
            fn=handle_test_embedding_config,
            inputs=[
                embedding_interface, embedding_api_key,
                embedding_base_url, embedding_model, config_log_output
            ],
            outputs=config_log_output
        )

        # æ¸…ç©ºé…ç½®æ—¥å¿—äº‹ä»¶
        btn_clear_config_log.click(
            fn=handle_clear_config_log,
            outputs=config_log_output
        )

        # æ ¸å¿ƒç”ŸæˆåŠŸèƒ½äº‹ä»¶
        btn_step1.click(
            fn=handle_generate_architecture,
            inputs=[
                llm_interface, llm_api_key, llm_base_url,
                llm_model, temperature, max_tokens, timeout,
                topic_input, genre_input, num_chapters_input,
                word_number_input, filepath_input, user_guidance_input,
                log_output
            ],
            outputs=log_output
        )

        btn_step2.click(
            fn=handle_generate_blueprint,
            inputs=[
                llm_interface, llm_api_key, llm_base_url,
                llm_model, temperature, max_tokens, timeout,
                filepath_input, log_output
            ],
            outputs=log_output
        )

        btn_step3.click(
            fn=handle_generate_chapter_draft,
            inputs=[
                llm_interface, llm_api_key, llm_base_url,
                llm_model, temperature, max_tokens, timeout,
                embedding_interface, embedding_api_key, embedding_base_url,
                embedding_model, retrieval_k,
                filepath_input, chapter_num_input, user_guidance_input,
                log_output
            ],
            outputs=[chapter_content, log_output]
        )

        btn_step4.click(
            fn=handle_finalize_chapter,
            inputs=[
                llm_interface, llm_api_key, llm_base_url,
                llm_model, temperature, max_tokens, timeout,
                embedding_interface, embedding_api_key, embedding_base_url,
                embedding_model,
                filepath_input, chapter_num_input, chapter_content,
                log_output
            ],
            outputs=log_output
        )

        # è¾…åŠ©åŠŸèƒ½äº‹ä»¶
        btn_consistency.click(
            fn=handle_consistency_check,
            inputs=[
                llm_interface, llm_api_key, llm_base_url,
                llm_model, temperature, max_tokens, timeout,
                filepath_input, chapter_num_input, log_output
            ],
            outputs=log_output
        )

        btn_import_knowledge.click(
            fn=handle_import_knowledge,
            inputs=[filepath_input, log_output],
            outputs=log_output
        )

        btn_clear_vectorstore.click(
            fn=handle_clear_vectorstore,
            inputs=[filepath_input, log_output],
            outputs=log_output
        )

        btn_plot_arcs.click(
            fn=handle_show_plot_arcs,
            inputs=[filepath_input, log_output],
            outputs=log_output
        )

        # æ–‡ä»¶ç®¡ç†äº‹ä»¶
        btn_load_architecture.click(
            fn=lambda filepath: handle_load_file(filepath, "Novel_architecture.txt"),
            inputs=filepath_input,
            outputs=[architecture_content, log_output]
        )

        btn_save_architecture.click(
            fn=lambda filepath, content: handle_save_file(filepath, "Novel_architecture.txt", content),
            inputs=[filepath_input, architecture_content],
            outputs=log_output
        )

        btn_load_blueprint.click(
            fn=lambda filepath: handle_load_file(filepath, "Novel_directory.txt"),
            inputs=filepath_input,
            outputs=[blueprint_content, log_output]
        )

        btn_save_blueprint.click(
            fn=lambda filepath, content: handle_save_file(filepath, "Novel_directory.txt", content),
            inputs=[filepath_input, blueprint_content],
            outputs=log_output
        )

        btn_load_character.click(
            fn=lambda filepath: handle_load_file(filepath, "character_state.txt"),
            inputs=filepath_input,
            outputs=[character_content, log_output]
        )

        btn_save_character.click(
            fn=lambda filepath, content: handle_save_file(filepath, "character_state.txt", content),
            inputs=[filepath_input, character_content],
            outputs=log_output
        )

        btn_load_summary.click(
            fn=lambda filepath: handle_load_file(filepath, "global_summary.txt"),
            inputs=filepath_input,
            outputs=[summary_content, log_output]
        )

        btn_save_summary.click(
            fn=lambda filepath, content: handle_save_file(filepath, "global_summary.txt", content),
            inputs=[filepath_input, summary_content],
            outputs=log_output
        )

        return demo

# äº‹ä»¶å¤„ç†å‡½æ•°
def handle_load_config():
    """å¤„ç†åŠ è½½é…ç½®äº‹ä»¶"""
    config_data, message = app.load_config_from_file()
    if config_data:
        return (
            config_data["llm_interface"],
            config_data["llm_api_key"],
            config_data["llm_base_url"],
            config_data["llm_model"],
            config_data["temperature"],
            config_data["max_tokens"],
            config_data["timeout"],
            config_data["embedding_interface"],
            config_data["embedding_api_key"],
            config_data["embedding_base_url"],
            config_data["embedding_model"],
            config_data["retrieval_k"],
            config_data["topic"],
            config_data["genre"],
            config_data["num_chapters"],
            config_data["word_number"],
            config_data["filepath"],
            config_data["user_guidance"],
            config_data["characters_involved"],
            config_data["key_items"],
            config_data["scene_location"],
            config_data["time_constraint"],
            message
        )
    else:
        return (
            "OpenAI", "", "https://api.openai.com/v1", "gpt-4o-mini", 0.7, 8192, 600,
            "OpenAI", "", "https://api.openai.com/v1", "text-embedding-ada-002", 4,
            "", "ç„å¹»", 10, 3000, "", "", "", "", "", "",
            message
        )

def handle_save_config(llm_interface, llm_api_key, llm_base_url, llm_model, temperature, max_tokens, timeout,
                      embedding_interface, embedding_api_key, embedding_base_url, embedding_model, retrieval_k,
                      topic, genre, num_chapters, word_number, filepath, user_guidance,
                      characters_involved, key_items, scene_location, time_constraint):
    """å¤„ç†ä¿å­˜é…ç½®äº‹ä»¶"""
    llm_config = app.get_llm_config_from_form(
        llm_interface, llm_api_key, llm_base_url, llm_model, temperature, max_tokens, timeout
    )
    embedding_config = app.get_embedding_config_from_form(
        embedding_interface, embedding_api_key, embedding_base_url, embedding_model, retrieval_k
    )
    novel_params = {
        "topic": topic,
        "genre": genre,
        "num_chapters": num_chapters,
        "word_number": word_number,
        "filepath": filepath,
        "user_guidance": user_guidance,
        "characters_involved": characters_involved,
        "key_items": key_items,
        "scene_location": scene_location,
        "time_constraint": time_constraint
    }

    return app.save_config_to_file(llm_config, embedding_config, novel_params)

def handle_test_llm_config(interface_format, api_key, base_url, model_name, temperature, max_tokens, timeout, current_log):
    """å¤„ç†æµ‹è¯•LLMé…ç½®äº‹ä»¶"""
    import datetime
    timestamp = datetime.datetime.now().strftime("%H:%M:%S")

    log_msg = current_log + f"\n[{timestamp}] ğŸ§ª å¼€å§‹æµ‹è¯•LLMé…ç½®...\n"
    log_msg += f"æ¥å£ç±»å‹: {interface_format}\n"
    log_msg += f"æ¨¡å‹åç§°: {model_name}\n"
    log_msg += f"Base URL: {base_url}\n"
    log_msg += f"Temperature: {temperature}\n"
    log_msg += f"Max Tokens: {max_tokens}\n"
    log_msg += f"Timeout: {timeout}ç§’\n"
    log_msg += "-" * 50 + "\n"

    try:
        log_msg += "æ­£åœ¨åˆ›å»ºLLMé€‚é…å™¨...\n"
        llm_adapter = create_llm_adapter(
            interface_format=interface_format,
            base_url=base_url,
            model_name=model_name,
            api_key=api_key,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout
        )
        log_msg += "âœ… LLMé€‚é…å™¨åˆ›å»ºæˆåŠŸ\n"

        test_prompt = "Please reply 'OK'"
        log_msg += f"å‘é€æµ‹è¯•æç¤º: {test_prompt}\n"
        log_msg += "ç­‰å¾…æ¨¡å‹å“åº”...\n"

        response = llm_adapter.invoke(test_prompt)
        if response and response.strip():
            log_msg += f"âœ… LLMé…ç½®æµ‹è¯•æˆåŠŸï¼\n"
            log_msg += f"æ¨¡å‹å›å¤: {response}\n"
            log_msg += f"å›å¤é•¿åº¦: {len(response)} å­—ç¬¦\n"
        else:
            log_msg += "âŒ LLMé…ç½®æµ‹è¯•å¤±è´¥ï¼šæœªè·å–åˆ°å“åº”\n"
            log_msg += "å¯èƒ½åŸå› ï¼š\n"
            log_msg += "1. APIå¯†é’¥æ— æ•ˆ\n"
            log_msg += "2. ç½‘ç»œè¿æ¥é—®é¢˜\n"
            log_msg += "3. æ¨¡å‹åç§°é”™è¯¯\n"
            log_msg += "4. æœåŠ¡å™¨æš‚æ—¶ä¸å¯ç”¨\n"

    except Exception as e:
        log_msg += f"âŒ LLMé…ç½®æµ‹è¯•å‡ºé”™: {str(e)}\n"
        log_msg += "è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n"
        import traceback
        log_msg += traceback.format_exc() + "\n"

    log_msg += "=" * 50 + "\n"
    return log_msg

def handle_test_embedding_config(interface_format, api_key, base_url, model_name, current_log):
    """å¤„ç†æµ‹è¯•Embeddingé…ç½®äº‹ä»¶"""
    import datetime
    timestamp = datetime.datetime.now().strftime("%H:%M:%S")

    log_msg = current_log + f"\n[{timestamp}] ğŸ” å¼€å§‹æµ‹è¯•Embeddingé…ç½®...\n"
    log_msg += f"æ¥å£ç±»å‹: {interface_format}\n"
    log_msg += f"æ¨¡å‹åç§°: {model_name}\n"
    log_msg += f"Base URL: {base_url}\n"
    log_msg += "-" * 50 + "\n"

    try:
        log_msg += "æ­£åœ¨åˆ›å»ºEmbeddingé€‚é…å™¨...\n"
        embedding_adapter = create_embedding_adapter(
            interface_format=interface_format,
            api_key=api_key,
            base_url=base_url,
            model_name=model_name
        )
        log_msg += "âœ… Embeddingé€‚é…å™¨åˆ›å»ºæˆåŠŸ\n"

        test_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬"
        log_msg += f"æµ‹è¯•æ–‡æœ¬: {test_text}\n"
        log_msg += "æ­£åœ¨ç”Ÿæˆå‘é‡...\n"

        embeddings = embedding_adapter.embed_query(test_text)
        if embeddings and len(embeddings) > 0:
            log_msg += f"âœ… Embeddingé…ç½®æµ‹è¯•æˆåŠŸï¼\n"
            log_msg += f"ç”Ÿæˆçš„å‘é‡ç»´åº¦: {len(embeddings)}\n"
            log_msg += f"å‘é‡å‰5ä¸ªå€¼: {embeddings[:5]}\n"
        else:
            log_msg += "âŒ Embeddingé…ç½®æµ‹è¯•å¤±è´¥ï¼šæœªè·å–åˆ°å‘é‡\n"
            log_msg += "å¯èƒ½åŸå› ï¼š\n"
            log_msg += "1. APIå¯†é’¥æ— æ•ˆ\n"
            log_msg += "2. æ¨¡å‹åç§°é”™è¯¯\n"
            log_msg += "3. ç½‘ç»œè¿æ¥é—®é¢˜\n"
            log_msg += "4. æœåŠ¡ä¸æ”¯æŒè¯¥æ¨¡å‹\n"

    except Exception as e:
        log_msg += f"âŒ Embeddingé…ç½®æµ‹è¯•å‡ºé”™: {str(e)}\n"
        log_msg += "è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n"
        import traceback
        log_msg += traceback.format_exc() + "\n"

    log_msg += "=" * 50 + "\n"
    return log_msg

def handle_clear_config_log():
    """æ¸…ç©ºé…ç½®æ—¥å¿—"""
    import datetime
    timestamp = datetime.datetime.now().strftime("%H:%M:%S")
    return f"[{timestamp}] ğŸ“‹ é…ç½®æµ‹è¯•æ—¥å¿—å·²æ¸…ç©º\n"

def handle_load_file(filepath, filename):
    """å¤„ç†æ–‡ä»¶åŠ è½½äº‹ä»¶"""
    if not filepath:
        return "", "âŒ è¯·å…ˆè®¾ç½®ä¿å­˜æ–‡ä»¶è·¯å¾„"

    full_path = os.path.join(filepath, filename)
    content = read_file(full_path)
    if content:
        return content, f"âœ… å·²åŠ è½½ {filename}"
    else:
        return "", f"âŒ æ— æ³•åŠ è½½ {filename}"

def handle_save_file(filepath, filename, content):
    """å¤„ç†æ–‡ä»¶ä¿å­˜äº‹ä»¶"""
    if not filepath:
        return "âŒ è¯·å…ˆè®¾ç½®ä¿å­˜æ–‡ä»¶è·¯å¾„"

    full_path = os.path.join(filepath, filename)
    try:
        clear_file_content(full_path)
        save_string_to_txt(content, full_path)
        return f"âœ… å·²ä¿å­˜ {filename}"
    except Exception as e:
        return f"âŒ ä¿å­˜ {filename} æ—¶å‡ºé”™: {str(e)}"

# æ ¸å¿ƒç”ŸæˆåŠŸèƒ½å¤„ç†å‡½æ•°
def handle_generate_architecture(llm_interface, llm_api_key, llm_base_url, llm_model, temperature, max_tokens, timeout,
                                topic, genre, num_chapters, word_number, filepath, user_guidance, current_log):
    """å¤„ç†ç”Ÿæˆå°è¯´æ¶æ„äº‹ä»¶"""
    if not filepath:
        return current_log + app.log_message("âŒ è¯·å…ˆè®¾ç½®ä¿å­˜æ–‡ä»¶è·¯å¾„")

    if not topic.strip():
        return current_log + app.log_message("âŒ è¯·å…ˆè¾“å…¥å°è¯´ä¸»é¢˜")

    try:
        log_msg = current_log + app.log_message("ğŸš€ å¼€å§‹ç”Ÿæˆå°è¯´æ¶æ„...")

        # åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œç”Ÿæˆ
        def generate_task():
            try:
                Novel_architecture_generate(
                    interface_format=llm_interface,
                    api_key=llm_api_key,
                    base_url=llm_base_url,
                    llm_model=llm_model,
                    topic=topic,
                    genre=genre,
                    number_of_chapters=int(num_chapters),
                    word_number=int(word_number),
                    filepath=filepath,
                    user_guidance=user_guidance,
                    temperature=temperature,
                    max_tokens=int(max_tokens),
                    timeout=int(timeout)
                )
                return "âœ… å°è¯´æ¶æ„ç”Ÿæˆå®Œæˆï¼"
            except Exception as e:
                return f"âŒ ç”Ÿæˆå°è¯´æ¶æ„æ—¶å‡ºé”™: {str(e)}"

        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥ä½¿ç”¨å¼‚æ­¥æˆ–è¿›åº¦æ¡
        result = generate_task()
        return log_msg + app.log_message(result)

    except Exception as e:
        return current_log + app.log_message(f"âŒ ç”Ÿæˆå°è¯´æ¶æ„æ—¶å‡ºé”™: {str(e)}")

def handle_generate_blueprint(llm_interface, llm_api_key, llm_base_url, llm_model, temperature, max_tokens, timeout,
                             filepath, current_log):
    """å¤„ç†ç”Ÿæˆç« èŠ‚è“å›¾äº‹ä»¶"""
    if not filepath:
        return current_log + app.log_message("âŒ è¯·å…ˆè®¾ç½®ä¿å­˜æ–‡ä»¶è·¯å¾„")

    try:
        log_msg = current_log + app.log_message("ğŸš€ å¼€å§‹ç”Ÿæˆç« èŠ‚è“å›¾...")

        def generate_task():
            try:
                Chapter_blueprint_generate(
                    interface_format=llm_interface,
                    api_key=llm_api_key,
                    base_url=llm_base_url,
                    llm_model=llm_model,
                    filepath=filepath,
                    temperature=temperature,
                    max_tokens=int(max_tokens),
                    timeout=int(timeout)
                )
                return "âœ… ç« èŠ‚è“å›¾ç”Ÿæˆå®Œæˆï¼"
            except Exception as e:
                return f"âŒ ç”Ÿæˆç« èŠ‚è“å›¾æ—¶å‡ºé”™: {str(e)}"

        result = generate_task()
        return log_msg + app.log_message(result)

    except Exception as e:
        return current_log + app.log_message(f"âŒ ç”Ÿæˆç« èŠ‚è“å›¾æ—¶å‡ºé”™: {str(e)}")

def handle_generate_chapter_draft(llm_interface, llm_api_key, llm_base_url, llm_model, temperature, max_tokens, timeout,
                                 embedding_interface, embedding_api_key, embedding_base_url, embedding_model, retrieval_k,
                                 filepath, chapter_num, user_guidance, current_log):
    """å¤„ç†ç”Ÿæˆç« èŠ‚è‰ç¨¿äº‹ä»¶"""
    if not filepath:
        return "", current_log + app.log_message("âŒ è¯·å…ˆè®¾ç½®ä¿å­˜æ–‡ä»¶è·¯å¾„")

    try:
        log_msg = current_log + app.log_message(f"ğŸš€ å¼€å§‹ç”Ÿæˆç¬¬{chapter_num}ç« è‰ç¨¿...")

        def generate_task():
            try:
                result = generate_chapter_draft(
                    interface_format=llm_interface,
                    api_key=llm_api_key,
                    base_url=llm_base_url,
                    llm_model=llm_model,
                    embedding_interface_format=embedding_interface,
                    embedding_api_key=embedding_api_key,
                    embedding_base_url=embedding_base_url,
                    embedding_model_name=embedding_model,
                    retrieval_k=int(retrieval_k),
                    filepath=filepath,
                    chapter_num=int(chapter_num),
                    user_guidance=user_guidance,
                    temperature=temperature,
                    max_tokens=int(max_tokens),
                    timeout=int(timeout)
                )

                # è¯»å–ç”Ÿæˆçš„ç« èŠ‚å†…å®¹
                chapter_file = os.path.join(filepath, f"chapter_{chapter_num}.txt")
                chapter_content = read_file(chapter_file)

                return chapter_content, "âœ… ç« èŠ‚è‰ç¨¿ç”Ÿæˆå®Œæˆï¼"
            except Exception as e:
                return "", f"âŒ ç”Ÿæˆç« èŠ‚è‰ç¨¿æ—¶å‡ºé”™: {str(e)}"

        chapter_content, result_msg = generate_task()
        return chapter_content, log_msg + app.log_message(result_msg)

    except Exception as e:
        return "", current_log + app.log_message(f"âŒ ç”Ÿæˆç« èŠ‚è‰ç¨¿æ—¶å‡ºé”™: {str(e)}")

def handle_finalize_chapter(llm_interface, llm_api_key, llm_base_url, llm_model, temperature, max_tokens, timeout,
                           embedding_interface, embedding_api_key, embedding_base_url, embedding_model,
                           filepath, chapter_num, chapter_content, current_log):
    """å¤„ç†å®šç¨¿ç« èŠ‚äº‹ä»¶"""
    if not filepath:
        return current_log + app.log_message("âŒ è¯·å…ˆè®¾ç½®ä¿å­˜æ–‡ä»¶è·¯å¾„")

    if not chapter_content.strip():
        return current_log + app.log_message("âŒ ç« èŠ‚å†…å®¹ä¸ºç©ºï¼Œæ— æ³•å®šç¨¿")

    try:
        log_msg = current_log + app.log_message(f"ğŸš€ å¼€å§‹å®šç¨¿ç¬¬{chapter_num}ç« ...")

        # å…ˆä¿å­˜å½“å‰ç« èŠ‚å†…å®¹
        chapter_file = os.path.join(filepath, f"chapter_{chapter_num}.txt")
        save_string_to_txt(chapter_content, chapter_file)

        def finalize_task():
            try:
                finalize_chapter(
                    interface_format=llm_interface,
                    api_key=llm_api_key,
                    base_url=llm_base_url,
                    llm_model=llm_model,
                    embedding_interface_format=embedding_interface,
                    embedding_api_key=embedding_api_key,
                    embedding_base_url=embedding_base_url,
                    embedding_model_name=embedding_model,
                    filepath=filepath,
                    chapter_num=int(chapter_num),
                    temperature=temperature,
                    max_tokens=int(max_tokens),
                    timeout=int(timeout)
                )
                return "âœ… ç« èŠ‚å®šç¨¿å®Œæˆï¼å·²æ›´æ–°å…¨å±€æ‘˜è¦ã€è§’è‰²çŠ¶æ€å’Œå‘é‡åº“ã€‚"
            except Exception as e:
                return f"âŒ å®šç¨¿ç« èŠ‚æ—¶å‡ºé”™: {str(e)}"

        result = finalize_task()
        return log_msg + app.log_message(result)

    except Exception as e:
        return current_log + app.log_message(f"âŒ å®šç¨¿ç« èŠ‚æ—¶å‡ºé”™: {str(e)}")

# è¾…åŠ©åŠŸèƒ½å¤„ç†å‡½æ•°
def handle_consistency_check(llm_interface, llm_api_key, llm_base_url, llm_model, temperature, max_tokens, timeout,
                            filepath, chapter_num, current_log):
    """å¤„ç†ä¸€è‡´æ€§æ£€æŸ¥äº‹ä»¶"""
    if not filepath:
        return current_log + app.log_message("âŒ è¯·å…ˆè®¾ç½®ä¿å­˜æ–‡ä»¶è·¯å¾„")

    try:
        log_msg = current_log + app.log_message(f"ğŸ” å¼€å§‹æ£€æŸ¥ç¬¬{chapter_num}ç« çš„ä¸€è‡´æ€§...")

        def check_task():
            try:
                result = check_consistency(
                    interface_format=llm_interface,
                    api_key=llm_api_key,
                    base_url=llm_base_url,
                    llm_model=llm_model,
                    filepath=filepath,
                    chapter_num=int(chapter_num),
                    temperature=temperature,
                    max_tokens=int(max_tokens),
                    timeout=int(timeout)
                )
                return f"âœ… ä¸€è‡´æ€§æ£€æŸ¥å®Œæˆï¼\n{result}"
            except Exception as e:
                return f"âŒ ä¸€è‡´æ€§æ£€æŸ¥æ—¶å‡ºé”™: {str(e)}"

        result = check_task()
        return log_msg + app.log_message(result)

    except Exception as e:
        return current_log + app.log_message(f"âŒ ä¸€è‡´æ€§æ£€æŸ¥æ—¶å‡ºé”™: {str(e)}")

def handle_import_knowledge(filepath, current_log):
    """å¤„ç†å¯¼å…¥çŸ¥è¯†åº“äº‹ä»¶"""
    if not filepath:
        return current_log + app.log_message("âŒ è¯·å…ˆè®¾ç½®ä¿å­˜æ–‡ä»¶è·¯å¾„")

    try:
        log_msg = current_log + app.log_message("ğŸ“š å¼€å§‹å¯¼å…¥çŸ¥è¯†åº“...")

        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥æä¾›æ–‡ä»¶é€‰æ‹©ç•Œé¢
        knowledge_file = os.path.join(filepath, "knowledge.txt")
        if os.path.exists(knowledge_file):
            import_knowledge_file(knowledge_file, filepath)
            return log_msg + app.log_message("âœ… çŸ¥è¯†åº“å¯¼å…¥å®Œæˆï¼")
        else:
            return log_msg + app.log_message(f"âŒ æœªæ‰¾åˆ°çŸ¥è¯†åº“æ–‡ä»¶: {knowledge_file}")

    except Exception as e:
        return current_log + app.log_message(f"âŒ å¯¼å…¥çŸ¥è¯†åº“æ—¶å‡ºé”™: {str(e)}")

def handle_clear_vectorstore(filepath, current_log):
    """å¤„ç†æ¸…ç©ºå‘é‡åº“äº‹ä»¶"""
    if not filepath:
        return current_log + app.log_message("âŒ è¯·å…ˆè®¾ç½®ä¿å­˜æ–‡ä»¶è·¯å¾„")

    try:
        log_msg = current_log + app.log_message("ğŸ—‘ï¸ å¼€å§‹æ¸…ç©ºå‘é‡åº“...")
        clear_vector_store(filepath)
        return log_msg + app.log_message("âœ… å‘é‡åº“å·²æ¸…ç©ºï¼")

    except Exception as e:
        return current_log + app.log_message(f"âŒ æ¸…ç©ºå‘é‡åº“æ—¶å‡ºé”™: {str(e)}")

def handle_show_plot_arcs(filepath, current_log):
    """å¤„ç†æŸ¥çœ‹å‰§æƒ…è¦ç‚¹äº‹ä»¶"""
    if not filepath:
        return current_log + app.log_message("âŒ è¯·å…ˆè®¾ç½®ä¿å­˜æ–‡ä»¶è·¯å¾„")

    try:
        plot_arcs_file = os.path.join(filepath, "plot_arcs.txt")
        content = read_file(plot_arcs_file)
        if content:
            return current_log + app.log_message(f"ğŸ“– å‰§æƒ…è¦ç‚¹å†…å®¹ï¼š\n{content}")
        else:
            return current_log + app.log_message("âŒ æœªæ‰¾åˆ°å‰§æƒ…è¦ç‚¹æ–‡ä»¶")

    except Exception as e:
        return current_log + app.log_message(f"âŒ æŸ¥çœ‹å‰§æƒ…è¦ç‚¹æ—¶å‡ºé”™: {str(e)}")

# åˆ é™¤é‡å¤çš„äº‹ä»¶å¤„ç†å™¨å‡½æ•°ï¼Œå·²ç»åœ¨create_interfaceä¸­ç›´æ¥è®¾ç½®

if __name__ == "__main__":
    demo = create_interface()

    print("ğŸš€ å¯åŠ¨AIå°è¯´ç”Ÿæˆå™¨Webç•Œé¢...")
    print("ğŸ“ è®¿é—®åœ°å€: http://localhost:7860")

    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True
    )